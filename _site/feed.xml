<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-08-22T13:28:14+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Abonia Sojasingarayar</title><subtitle>Holds a Masters degree in Artificial Intelligence and boasts over 7 years of experience. With a robust background in crafting and implementing ML systems across diverse domains, including cybersecurity, banking, transit,cosmetics &amp; hygiene research, and e-commerce.
</subtitle><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><entry><title type="html">Deploying a Scalable Machine Learning Service on Kubernetes</title><link href="http://localhost:4000/blogs/2025-05-01-Deploying-Scalable-Machine-Learning-Service-on-Kubernetes/" rel="alternate" type="text/html" title="Deploying a Scalable Machine Learning Service on Kubernetes" /><published>2025-05-01T00:00:00+02:00</published><updated>2025-08-11T13:04:00+02:00</updated><id>http://localhost:4000/blogs/Deploying-Scalable-Machine-Learning-Service-on-Kubernetes</id><content type="html" xml:base="http://localhost:4000/blogs/2025-05-01-Deploying-Scalable-Machine-Learning-Service-on-Kubernetes/"><![CDATA[<p><strong>Reading_time:</strong> 15 min<br />
  <strong>Tags:</strong> [MLOps, Kubernetes, Machine Learning, FastAPI, Docker, Podman, Scikit-Learn, Sentiment Analysis, API Deployment, Model Serving, Auto-scaling, HPA, Prometheus, Monitoring, Containerization, Dev]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#a-comprehensive-tutorial-on-deploying-a-scalable-machine-learning-service-on-kubernetes" id="markdown-toc-a-comprehensive-tutorial-on-deploying-a-scalable-machine-learning-service-on-kubernetes">A Comprehensive Tutorial on Deploying a Scalable Machine Learning Service on Kubernetes</a></li>
  <li><a href="#overview-of-the-tutorial" id="markdown-toc-overview-of-the-tutorial">Overview of the Tutorial</a></li>
  <li><a href="#what-youll-learn" id="markdown-toc-what-youll-learn">What You’ll Learn</a></li>
  <li><a href="#technical-stack" id="markdown-toc-technical-stack">Technical Stack</a></li>
  <li><a href="#topics-covered" id="markdown-toc-topics-covered">Topics Covered</a>    <ul>
      <li><a href="#introduction--project-overview" id="markdown-toc-introduction--project-overview">Introduction &amp; Project Overview</a></li>
      <li><a href="#setting-up-podman--kind" id="markdown-toc-setting-up-podman--kind">Setting Up Podman &amp; Kind</a></li>
      <li><a href="#creating-a-kubernetes-cluster" id="markdown-toc-creating-a-kubernetes-cluster">Creating a Kubernetes Cluster</a></li>
      <li><a href="#deploying-persistent-storage" id="markdown-toc-deploying-persistent-storage">Deploying Persistent Storage</a></li>
      <li><a href="#setting-up-configmap" id="markdown-toc-setting-up-configmap">Setting Up ConfigMap</a></li>
      <li><a href="#deploying-the-ml-application" id="markdown-toc-deploying-the-ml-application">Deploying the ML Application</a></li>
      <li><a href="#exposing-the-service--auto-scaling" id="markdown-toc-exposing-the-service--auto-scaling">Exposing the Service &amp; Auto-Scaling</a></li>
      <li><a href="#setting-up-prometheus-for-monitoring" id="markdown-toc-setting-up-prometheus-for-monitoring">Setting Up Prometheus for Monitoring</a></li>
      <li><a href="#testing-the-api--metrics" id="markdown-toc-testing-the-api--metrics">Testing the API &amp; Metrics</a></li>
      <li><a href="#debugging--troubleshooting" id="markdown-toc-debugging--troubleshooting">Debugging &amp; Troubleshooting</a></li>
      <li><a href="#conclusion--next-steps" id="markdown-toc-conclusion--next-steps">Conclusion &amp; Next Steps</a></li>
    </ul>
  </li>
  <li><a href="#video-tutorial-series" id="markdown-toc-video-tutorial-series">Video Tutorial Series</a></li>
  <li><a href="#resources-used" id="markdown-toc-resources-used">Resources Used</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>

<h2 id="a-comprehensive-tutorial-on-deploying-a-scalable-machine-learning-service-on-kubernetes">A Comprehensive Tutorial on Deploying a Scalable Machine Learning Service on Kubernetes</h2>

<iframe width="900" height="500" src="https://www.youtube.com/embed/hlntSaGY-dQ" frameborder="0" allowfullscreen=""></iframe>

<p><strong>GitHub Repository</strong>: <a href="https://github.com/Abonia1/kubernetes-ml-project">kubernetes-ml-deployment</a></p>

<h2 id="overview-of-the-tutorial">Overview of the Tutorial</h2>

<p>This tutorial covers the entire pipeline of deploying a Sentiment Analysis model — from model development to scalable, monitored production deployment on Kubernetes.</p>

<p><strong>Here’s a glimpse of what you will build and learn:</strong></p>

<ul>
  <li>Sentiment Analysis Model developed with Scikit-Learn for natural language processing tasks.</li>
  <li>A FastAPI-based REST API serving the model, enabling easy, low-latency inference.</li>
  <li>Containerization using Docker or Podman to package the application and dependencies.</li>
  <li>Kubernetes Deployment, including setup of clusters, persistent storage for model artifacts, and auto-scaling using the Horizontal Pod Autoscaler (HPA).</li>
  <li>Real-time monitoring using Prometheus to track application health and performance metrics.</li>
</ul>

<p>This project equips you with practical MLOps skills to deploy and maintain machine learning applications at scale.</p>

<hr />

<h2 id="what-youll-learn">What You’ll Learn</h2>
<p><img src="/assets/img/blog/k8s.png" alt="Image- Author" /></p>

<ul>
  <li>Building and training a sentiment analysis model with Scikit-Learn.</li>
  <li>Designing a RESTful API using FastAPI for seamless model inference.</li>
  <li>Containerizing ML services with Docker or Podman for portability.</li>
  <li>Creating and managing Kubernetes clusters with Kind (Kubernetes in Docker).</li>
  <li>Configuring Persistent Volumes (PV) and Persistent Volume Claims (PVC) for reliable storage.</li>
  <li>Managing application configurations with Kubernetes ConfigMaps.</li>
  <li>Implementing Horizontal Pod Autoscaling (HPA) for dynamic load management.</li>
  <li>Setting up Prometheus monitoring for real-time insights and alerts.</li>
  <li>Testing APIs and Prometheus metrics along with common debugging practices.</li>
</ul>

<hr />

<h2 id="technical-stack">Technical Stack</h2>

<ul>
  <li><strong>Machine Learning</strong>: Scikit-Learn</li>
  <li><strong>API Framework</strong>: FastAPI</li>
  <li><strong>Containerization</strong>: Docker, Podman</li>
  <li><strong>Orchestration</strong>: Kubernetes, Kind</li>
  <li><strong>Storage</strong>: Persistent Volumes in Kubernetes</li>
  <li><strong>Auto-scaling</strong>: Horizontal Pod Autoscaler (HPA)</li>
  <li><strong>Monitoring</strong>: Prometheus</li>
</ul>

<hr />

<h2 id="topics-covered">Topics Covered</h2>

<h3 id="introduction--project-overview">Introduction &amp; Project Overview</h3>
<ul>
  <li>Understand the scope and architecture of the deployment pipeline.</li>
</ul>

<h3 id="setting-up-podman--kind">Setting Up Podman &amp; Kind</h3>
<ul>
  <li>Install and configure Podman and Kind for local Kubernetes cluster management.</li>
</ul>

<h3 id="creating-a-kubernetes-cluster">Creating a Kubernetes Cluster</h3>
<ul>
  <li>Initialize and configure your Kubernetes environment.</li>
</ul>

<h3 id="deploying-persistent-storage">Deploying Persistent Storage</h3>
<ul>
  <li>Configure Persistent Volumes and Claims for storing model data.</li>
</ul>

<h3 id="setting-up-configmap">Setting Up ConfigMap</h3>
<ul>
  <li>Manage application settings with Kubernetes ConfigMaps.</li>
</ul>

<h3 id="deploying-the-ml-application">Deploying the ML Application</h3>
<ul>
  <li>Containerize and deploy your ML model API on Kubernetes.</li>
</ul>

<h3 id="exposing-the-service--auto-scaling">Exposing the Service &amp; Auto-Scaling</h3>
<ul>
  <li>Make your service accessible and configure HPA to handle varying workloads.</li>
</ul>

<h3 id="setting-up-prometheus-for-monitoring">Setting Up Prometheus for Monitoring</h3>
<ul>
  <li>Integrate Prometheus to monitor system and application metrics.</li>
</ul>

<h3 id="testing-the-api--metrics">Testing the API &amp; Metrics</h3>
<ul>
  <li>Validate the deployed API and monitor Prometheus outputs.</li>
</ul>

<h3 id="debugging--troubleshooting">Debugging &amp; Troubleshooting</h3>
<ul>
  <li>Identify and resolve common issues encountered in production deployments.</li>
</ul>

<h3 id="conclusion--next-steps">Conclusion &amp; Next Steps</h3>
<ul>
  <li>Summarize learnings and suggest paths for further enhancement.</li>
</ul>

<hr />

<h2 id="video-tutorial-series">Video Tutorial Series</h2>

<p>Follow along with my step-by-step tutorial series on <strong>Hands-On End-to-End ML Model Deployment on Kubernetes - Auto-Scaling &amp; Monitoring</strong> playlist:</p>

<ul>
  <li><strong>Part 1</strong>: <a href="https://youtu.be/hlntSaGY-dQ">Introduction &amp; Project Setup</a></li>
</ul>
<iframe width="900" height="500" src="https://www.youtube.com/embed/hlntSaGY-dQ" frameborder="0" allowfullscreen=""></iframe>

<ul>
  <li><strong>Part 2</strong>: <a href="https://youtu.be/sKWZY0GJSuE">Setup Podman and Install Kind</a></li>
</ul>
<iframe width="900" height="500" src="https://www.youtube.com/embed/sKWZY0GJSuE" frameborder="0" allowfullscreen=""></iframe>

<ul>
  <li><strong>Part 3</strong>: <a href="https://youtu.be/pc6GCL41BXk">Building the Machine Learning Model &amp; API</a></li>
</ul>
<iframe width="900" height="500" src="https://www.youtube.com/embed/pc6GCL41BXk" frameborder="0" allowfullscreen=""></iframe>

<ul>
  <li><strong>Part 4</strong>: <a href="https://youtu.be/9mIu3DKJHhU">Containerization with Docker/Podman</a></li>
</ul>
<iframe width="900" height="500" src="https://www.youtube.com/embed/9mIu3DKJHhU" frameborder="0" allowfullscreen=""></iframe>

<ul>
  <li><strong>Part 5</strong>: <a href="https://youtu.be/_jqMRb6Mt-0">Setting Up Kubernetes Cluster and Deploying the ML Service</a></li>
</ul>
<iframe width="900" height="500" src="https://www.youtube.com/embed/_jqMRb6Mt-0" frameborder="0" allowfullscreen=""></iframe>

<ul>
  <li><strong>Part 6</strong>: <a href="https://youtu.be/ZG6BopWtvtE">Auto-Scaling with HPA and Monitoring with Prometheus</a></li>
</ul>
<iframe width="900" height="500" src="https://www.youtube.com/embed/ZG6BopWtvtE" frameborder="0" allowfullscreen=""></iframe>

<ul>
  <li><strong>Part 7</strong>: <a href="https://youtu.be/3IGqEX-1ZhM">Scalable Options, Testing, Debugging &amp; Optimization</a></li>
</ul>
<iframe width="900" height="500" src="https://www.youtube.com/embed/3IGqEX-1ZhM" frameborder="0" allowfullscreen=""></iframe>

<hr />

<h2 id="resources-used">Resources Used</h2>

<ul>
  <li><strong>GitHub Repository</strong>: <a href="https://github.com/Abonia1/kubernetes-ml-project">kubernetes-ml-deployment</a></li>
  <li><strong>Docker Hub Image</strong>: <a href="https://hub.docker.com/r/abonia/ml-tutorial"><code class="language-plaintext highlighter-rouge">abonia/ml-tutorial</code></a></li>
  <li><a href="https://podman.io/getting-started/installation">Podman Installation Guide</a></li>
  <li><a href="https://www.docker.com/products/docker-desktop/">Docker Desktop Download</a></li>
  <li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/">Kind (Kubernetes in Docker) Quick Start</a></li>
  <li><a href="https://kubernetes.io/docs/home/">Kubernetes Official Documentation</a></li>
  <li><a href="https://prometheus.io/docs/introduction/overview/">Prometheus Documentation</a></li>
  <li><a href="https://fastapi.tiangolo.com/">FastAPI Documentation</a></li>
  <li><a href="https://scikit-learn.org/stable/supervised_learning.html">Scikit-Learn Supervised Learning</a></li>
</ul>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>Mastering the deployment of machine learning models on Kubernetes unlocks the potential to build robust, scalable, and maintainable AI-powered applications. Whether you’re a beginner stepping into MLOps or a developer aiming to enhance your deployment pipeline, this tutorial offers practical insights to elevate your skills.</p>

<p>Stay connected for updates, more tutorials, and guides by subscribing to my <a href="https://abonia1.github.io/newsletter/">newsletter</a> and following on <a href="https://www.linkedin.com/in/aboniasojasingarayar/">LinkedIn</a> and <a href="https://github.com/Abonia1">GitHub</a>.</p>

<hr />
<p><strong><em>Thanks for Reading!</em></strong></p>

<blockquote>
  <p><strong><a href="https://abonia1.github.io/">Website/Newletter</a></strong>
<strong><a href="https://aboniasojasingarayar.substack.com">AIMagazine Substack</a></strong></p>
</blockquote>

<blockquote>
  <p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>
</blockquote>

<blockquote>
  <p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>
</blockquote>

<blockquote>
  <p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[In today's rapidly evolving tech landscape, deploying machine learning models efficiently and reliably in production environments is a critical skill. This tutorial series provides a hands-on, end-to-end guide for deploying a machine learning service on Kubernetes, designed specifically for beginners eager to master MLOps practices.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/k8s-infra.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/k8s-infra.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Building a Scalable data Processing Pipeline with Kafka and Docker Compose</title><link href="http://localhost:4000/blogs/2025-03-02-Kafka-Building-Scalable-data-Processing-Pipeline-with-Kafka-and-docker-compose/" rel="alternate" type="text/html" title="Building a Scalable data Processing Pipeline with Kafka and Docker Compose" /><published>2025-03-02T00:00:00+01:00</published><updated>2025-03-07T13:29:27+01:00</updated><id>http://localhost:4000/blogs/Kafka-Building-Scalable-data-Processing-Pipeline-with-Kafka-and-docker-compose</id><content type="html" xml:base="http://localhost:4000/blogs/2025-03-02-Kafka-Building-Scalable-data-Processing-Pipeline-with-Kafka-and-docker-compose/"><![CDATA[<p><strong>Reading_time:</strong> 10 min<br />
  <strong>Tags:</strong> [ApacheKafka, DockerCompose, Dataprocessing, ImageProcessing, DataPipeline, RealTimeProcessing, Python]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#comprehensive-guide--a-step-by-step-guide-to-efficient-asynchronous-kafka-image-processing" id="markdown-toc-comprehensive-guide--a-step-by-step-guide-to-efficient-asynchronous-kafka-image-processing">Comprehensive Guide — A Step-by-Step Guide to Efficient Asynchronous Kafka Image Processing</a></li>
</ul>

<h3 id="comprehensive-guide--a-step-by-step-guide-to-efficient-asynchronous-kafka-image-processing">Comprehensive Guide — A Step-by-Step Guide to Efficient Asynchronous Kafka Image Processing</h3>

<p>Building a scalable data processing pipeline is crucial for applications that require efficient handling of large volumes of images in real-time. In this tutorial, we’ll guide you through the process of setting up an image processing pipeline using Apache Kafka for message brokering and Docker Compose for container orchestration. By the end of this guide, you’ll have a robust system capable of processing images asynchronously, ensuring scalability and resilience.</p>

<iframe width="900" height="500" src="https://www.youtube.com/embed/VvOaA4uPa10" frameborder="0" allowfullscreen=""></iframe>

<p><strong>Table of Contents:</strong></p>

<ol>
  <li>
    <p>Introduction</p>
  </li>
  <li>
    <p>System Architecture Overview</p>
  </li>
  <li>
    <p>Setting Up the Development Environment</p>
  </li>
  <li>
    <p>Building and Containerizing the Application</p>
  </li>
  <li>
    <p>Configuring Apache Kafka</p>
  </li>
  <li>
    <p>Deploying with Docker Compose</p>
  </li>
  <li>
    <p>Running and Monitoring the Pipeline</p>
  </li>
  <li>
    <p>Conclusion and Next Steps</p>
  </li>
</ol>

<p>Complete code available in <strong><a href="https://github.com/Abonia1/Kafka-Image-Processing-Pipeline">here</a></strong></p>

<p><strong>1. Introduction</strong></p>

<p>In modern applications, processing images efficiently is essential, especially when dealing with large datasets or real-time requirements. A scalable image processing pipeline allows for the ingestion, processing, and storage of images in a manner that can handle increasing loads gracefully. By leveraging <strong><a href="https://kafka.apache.org/">Apache Kafka</a></strong>, <strong><a href="https://zookeeper.apache.org/">Zookeeper</a></strong> and <strong><a href="https://docs.docker.com/compose/">Docker Compose</a></strong>, we can create a system that is both scalable and easy to manage.</p>

<p><img src="https://cdn-images-1.medium.com/max/5376/1*-XADVwHmil0R4Ksk7IIMPQ.png" alt="Image- Author" /></p>

<p><strong>2. System Architecture Overview</strong></p>

<p>Our pipeline consists of the following components:</p>

<ul>
  <li>
    <p><strong>Image Source</strong>: The origin of images, which could be an application uploading images, a database, or an external service.</p>
  </li>
  <li>
    <p><strong>Kafka Producer</strong>: Receives images from the source and publishes them to a Kafka topic.</p>
  </li>
  <li>
    <p><strong>Kafka Broker</strong>: Acts as an intermediary, storing and forwarding messages from producers to consumers.</p>
  </li>
  <li>
    <p><strong>Image Processing Service</strong>: Consumes images from the Kafka topic, processes them (e.g., background removal, background replacement, upscaling), and stores the results.</p>
  </li>
  <li>
    <p><strong>Storage</strong>: A location to save the processed images, which could be a file system, database or cloud storage.</p>
  </li>
  <li>
    <p><strong>Monitoring and Logging</strong>: Tools to track the performance and health of the pipeline.</p>
  </li>
</ul>

<p><strong>3. Setting Up the Development Environment</strong></p>

<p>Before we begin, ensure you have the following installed:</p>

<ul>
  <li>
    <p><strong>Docker</strong>: For containerizing and running our services.</p>
  </li>
  <li>
    <p><strong>Docker Compose</strong>: To orchestrate multi-container Docker applications.</p>
  </li>
  <li>
    <p><strong>Python 3.x</strong>: For writing the image processing service.</p>
  </li>
  <li>
    <p><strong>Kafka-Python Library</strong>: To interact with Kafka from our Python application.</p>
  </li>
</ul>

<p><strong>4. Building and Containerizing the Application</strong></p>

<p>We’ll create a Python application that processes images by performing tasks such as background removal, background replacement, and upscaling. Each of these tasks can be handled by separate services or combined into a single service, depending on your requirements.</p>

<p><strong>a. Application Structure:</strong></p>

<ul>
  <li>
    <p><strong>api/app.py</strong>: The entry point of the application.</p>
  </li>
  <li>
    <p><strong>services/bg_removal_service.py</strong>: Handles background removal.</p>
  </li>
  <li>
    <p><strong>services/bg_replace_service.py</strong>: Handles background replacement.</p>
  </li>
  <li>
    <p><strong>services/upscale_service.py</strong>: Handles image upscaling.</p>
  </li>
  <li>
    <p><strong>utils/constants.py</strong>: Contains constant values used across the application.</p>
  </li>
</ul>

<p><strong>b. Dockerfile:</strong></p>

<p>We’ll create a Dockerfile to containerize our application:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Use the official Python image from the Docker Hub
FROM python:3.8-slim
# Set the working directory in the container
WORKDIR /app
# Copy the requirements file into the container
COPY requirements.txt .
# Install the required Python packages
RUN pip install --no-cache-dir -r requirements.txt
# Copy the rest of the application code into the container
COPY . .
# Run the application
CMD ["python", "main.py"]
</code></pre></div></div>

<p><strong>5. Configuring Apache Kafka</strong></p>

<p>Apache Kafka is a distributed streaming platform that allows for building real-time data pipelines. We’ll set up Kafka using Docker Compose.</p>

<p><strong>a. Docker Compose Configuration:</strong></p>

<p>Create a docker-compose.yml file to define our services:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version: '3.7'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    platform: linux/arm64
    container_name: zookeeper
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
    ports:
      - "2181:2181"
    networks:
      - kafka_network  
  
  kafka:
    image: confluentinc/cp-kafka:latest
    platform: linux/arm64
    container_name: kafka
    environment:
      - KAFKA_ADVERTISED_LISTENERS=INSIDE_KAFKA://kafka:9092,OUTSIDE_KAFKA://kafka:9093
      - KAFKA_ADVERTISED_LISTENERS=INSIDE_KAFKA://kafka:9092,OUTSIDE_KAFKA://localhost:9093
      - KAFKA_LISTENERS=INSIDE_KAFKA://0.0.0.0:9092,OUTSIDE_KAFKA://0.0.0.0:9093
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INSIDE_KAFKA:PLAINTEXT,OUTSIDE_KAFKA:PLAINTEXT
      - KAFKA_LISTENER_SECURITY_PROTOCOL=PLAINTEXT 
      - KAFKA_LISTENER_NAME_INSIDE_KAFKA_SECURITY_PROTOCOL=PLAINTEXT  
      - KAFKA_LISTENER_NAME_OUTSIDE_KAFKA_SECURITY_PROTOCOL=PLAINTEXT 
      - KAFKA_INTER_BROKER_LISTENER_NAME=INSIDE_KAFKA
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    volumes:
      - ./data/kafka:/var/lib/kafka/data 
    depends_on:
      - zookeeper 
    networks:
      - kafka_network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka:9093"]
      interval: 60s
      retries: 5

  image_processing_service:
    build:
      context: ..
      dockerfile: docker/Dockerfile  
    ports:
      - "5000:5000"
    depends_on:
      kafka:
        condition: service_healthy 
    environment:
      - KAFKA_BROKER=kafka:9093  # Internal broker address
    networks:
      - kafka_network

networks:
  kafka_network:
    driver: bridge
</code></pre></div></div>

<p><strong>6. Deploying with Docker Compose</strong></p>

<p>With our docker-compose.yml file in place, we can start our services:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up -d
</code></pre></div></div>

<p>This command will build and start the Zookeeper, Kafka, and application services in detached mode.</p>

<p><strong>7. Running and Monitoring the Pipeline</strong></p>

<p>Once the services are running, the application will begin consuming images from the Kafka topic, processing them, and storing the results.</p>

<p><strong><em>Check Topics</em></strong></p>

<p>List all topics in Kafka to ensure tasks are being published correctly:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker exec -it kafka kafka-topics - bootstrap-server kafka:9092 - list
</code></pre></div></div>

<p><strong>8. Monitoring:</strong></p>

<p>To monitor the pipeline, you can use Kafka’s command-line tools or set up a monitoring stack with <strong><a href="https://prometheus.io/">Prometheus</a></strong> and <strong><a href="https://grafana.com/">Grafana</a></strong>. For a comprehensive guide on setting up such a monitoring stack.
<strong><a href="https://www.confluent.io/blog/monitor-kafka-clusters-with-prometheus-grafana-and-confluent/">Monitor Apache Kafka Clusters with Prometheus, Grafana, and Confluent</a></strong></p>

<p>​<strong>9. Scalability: Designing for High Throughput</strong></p>

<p>To ensure that your image processing pipeline can handle increased workloads and maintain performance,consider the following strategies:</p>

<p>Refer <strong><a href="https://github.com/Abonia1/Kafka-Image-Processing-Pipeline/blob/main/docs/scalability_design.md">here</a></strong> for more scalability options:</p>

<p><strong>A. Horizontal Scaling</strong></p>

<p><strong>Kafka Cluster</strong></p>

<ul>
  <li>
    <p><strong>Cluster Deployment:</strong> Set up a <strong><a href="https://kafka.apache.org/">Kafka cluster</a></strong> with multiple brokers to manage higher message throughput.</p>
  </li>
  <li>
    <p><strong>Replication and Partitioning:</strong> Implement a replication factor for fault tolerance and partition topics to distribute the load across brokers.</p>
  </li>
</ul>

<p><strong>Application Instances</strong></p>

<ul>
  <li>
    <p><strong>Orchestration:</strong> Utilize orchestrators like <strong><a href="https://kubernetes.io/">Kubernetes</a></strong> or <strong><a href="https://docs.docker.com/engine/swarm/">Docker Swarm</a></strong> to deploy multiple instances of your application.</p>
  </li>
  <li>
    <p><strong>Consumer Groups:</strong> Configure these instances to subscribe to Kafka topics using consumer groups, enabling efficient distribution of message processing.</p>
  </li>
</ul>

<p><strong>B. Cloud Deployment</strong></p>

<p><strong>Managed Kafka Services</strong></p>

<ul>
  <li><strong>Service Selection:</strong> Opt for cloud-managed Kafka services such as <strong><a href="https://aws.amazon.com/msk/">AWS Managed Streaming for Apache Kafka (MSK)</a></strong>, <strong><a href="https://azure.microsoft.com/en-us/services/event-hubs/">Azure Event Hubs</a></strong>, or <strong><a href="https://www.confluent.io/confluent-cloud/">Confluent Cloud</a></strong> to minimize operational overhead and enhance reliability.</li>
</ul>

<p><strong>Serverless Compute</strong></p>

<p><strong>Processing Service Migration:</strong> Transition your processing services to serverless platforms like <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> or <a href="https://cloud.google.com/functions">Google Cloud Functions</a> to benefit from auto-scaling based on message volume.</p>

<p><strong>Load Balancing</strong></p>

<ul>
  <li><strong>Load Balancer Deployment:</strong> Implement a load balancer (e.g., <a href="https://aws.amazon.com/elasticloadbalancing/">AWS Elastic Load Balancing</a>, <a href="https://cloud.google.com/load-balancing">Google Cloud Load Balancing</a>) in front of your application API to manage and route incoming requests efficiently.</li>
</ul>

<p><strong>C. Optimized Message Processing</strong></p>

<p><strong>Batch Processing</strong></p>

<ul>
  <li><strong>Consumer Optimization:</strong> Implement batch processing for Kafka consumers to reduce the overhead of frequent I/O operations and improve throughput.</li>
</ul>

<p><strong>Message Filtering</strong></p>

<ul>
  <li><strong>Broker-Level Processing:</strong> Utilize <strong><a href="https://kafka.apache.org/documentation/streams/">Kafka Streams</a></strong> or <strong><a href="https://ksqldb.io/">ksqlDB</a></strong> to filter or preprocess messages at the broker level before they reach consumers.</li>
</ul>

<p><strong>Priority Queues</strong></p>

<ul>
  <li><strong>Topic Prioritization:</strong> Establish multiple Kafka topics with varying priorities for time-sensitive or critical tasks to ensure efficient processing.</li>
</ul>

<p><strong>D. Monitoring and Auto-Scaling</strong></p>

<p><strong>Monitoring Tools</strong></p>

<ul>
  <li><strong>Integration:</strong> Incorporate monitoring tools like <strong><a href="https://prometheus.io/">Prometheus</a></strong>, <strong><a href="https://grafana.com/">Grafana</a></strong>, or cloud-native solutions (e.g., <strong><a href="https://aws.amazon.com/cloudwatch/">AWS CloudWatch</a></strong>) to track system health and monitor Kafka message lag and broker performance.</li>
</ul>

<p><strong>Auto-Scaling Policies</strong></p>

<ul>
  <li><strong>Configuration:</strong> Set up auto-scaling policies for both Kafka brokers and application instances based on metrics such as CPU usage, memory utilization, and Kafka consumer lag.</li>
</ul>

<p><strong>E. Data Persistence</strong></p>

<p><strong>Data Lakes</strong></p>

<ul>
  <li><strong>Archiving:</strong> Use cloud-based storage solutions (e.g., <strong><a href="https://aws.amazon.com/s3/">Amazon S3</a>, <a href="https://cloud.google.com/storage">Google Cloud Storage</a></strong>) to archive processed images or messages for long-term retention and analytics.</li>
</ul>

<p><strong>Database Scaling</strong></p>

<ul>
  <li><strong>Metadata Storage:</strong> For metadata storage, migrate to a distributed database like <strong><a href="https://aws.amazon.com/rds/aurora/">Amazon Aurora</a></strong>, <strong><a href="https://www.mongodb.com/">MongoDB</a></strong>, or <strong><a href="https://cloud.google.com/spanner">Google Cloud Spanner</a></strong> to enable horizontal scalability.</li>
</ul>

<p><strong>F. Workflow Orchestration</strong></p>

<p><strong>Orchestrator Integration</strong></p>

<ul>
  <li><strong>Tool Selection:</strong> Integrate workflow orchestrators like <strong><a href="https://airflow.apache.org/">Apache Airflow</a></strong>, <strong><a href="https://www.prefect.io/">Prefect</a></strong>, or <strong><a href="https://argoproj.github.io/argo-workflows/">Argo Workflows</a></strong> to manage complex pipelines, handle task dependencies, retries, and scheduling for streamlined processing.</li>
</ul>

<p><img src="https://cdn-images-1.medium.com/max/2748/1*Ea0BHjLwHaXvlgMWtToX_Q.png" alt="Image- Author" /></p>

<p>By implementing these strategies, you can design an image processing pipeline that is both scalable and efficient, capable of handling high throughput in various deployment environments.</p>

<p><strong>Conclusion</strong></p>

<p>In this tutorial, we’ve walked through the process of building a scalable image processing pipeline using Apache Kafka for message brokering and Docker Compose for container orchestration. By leveraging these technologies, we’ve created a system capable of handling high-throughput image processing tasks efficiently.</p>

<p>Implementing monitoring and logging with tools like Prometheus and Grafana further enhances the pipeline’s robustness, allowing for proactive issue detection and performance optimization. This setup ensures that the system remains resilient and can scale to meet increasing demands.</p>

<p>As you continue to develop and optimize your image processing pipeline, consider exploring additional enhancements such as:</p>

<p><strong>Scaling Services:</strong> Implementing auto-scaling mechanisms to handle varying workloads.</p>

<p><strong>Advanced Monitoring:</strong> Setting up alerts and more detailed dashboards to monitor specific metrics.</p>

<p><strong>Security Measures:</strong> Ensuring secure communication between services and implementing authentication where necessary.</p>

<p>By continuously refining your pipeline and incorporating best practices, you’ll be well-equipped to handle complex image processing tasks in a scalable and efficient manner.</p>

<hr />
<p><strong><em>Thanks for Reading!</em></strong></p>

<blockquote>
  <p><strong><a href="https://abonia1.github.io/">Website/Newletter</a></strong>
<strong><a href="https://aboniasojasingarayar.substack.com">AIMagazine Substack</a></strong></p>
</blockquote>

<blockquote>
  <p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>
</blockquote>

<blockquote>
  <p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>
</blockquote>

<blockquote>
  <p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Building a scalable data processing pipeline is crucial for applications that require efficient handling of large volumes of data in real-time.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/kafka.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/kafka.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Join the Community in AI, ML, Data Science, ,Computer Vision, Data Science, GenAI, NLP, MLOps, LLMOps – Let’s Learn Together</title><link href="http://localhost:4000/blogs/2025-01-01-Join-the-Community-AI-Machine-Learning-ComputerVision-Data-Science-GenAI-NLP-MLOps-LLMOps/" rel="alternate" type="text/html" title="Join the Community in AI, ML, Data Science, ,Computer Vision, Data Science, GenAI, NLP, MLOps, LLMOps – Let’s Learn Together" /><published>2025-01-01T00:00:00+01:00</published><updated>2025-01-03T19:22:40+01:00</updated><id>http://localhost:4000/blogs/Join-the-Community-AI-Machine%20Learning-ComputerVision-Data%20Science-GenAI-NLP-MLOps-LLMOps</id><content type="html" xml:base="http://localhost:4000/blogs/2025-01-01-Join-the-Community-AI-Machine-Learning-ComputerVision-Data-Science-GenAI-NLP-MLOps-LLMOps/"><![CDATA[<p><strong>Reading_time:</strong> 5 min<br />
<strong>Tags:</strong> [AIJourney, LearnAI, MachineLearningForAll, DataScienceForBeginners, AICommunity, TechLearning, AIandML, GenerativeAI, AIExplained, LearnTogether, AIForEveryone, MLCommunity, DataScienceJourney, TechForGood, AIInsights]</p>

<iframe width="900" height="500" src="https://www.youtube.com/embed/D0-k6EnRM7E" frameborder="0" allowfullscreen=""></iframe>

<h3 id="lets-learn-together">Let’s Learn Together</h3>

<p>I’m thrilled to share something that’s incredibly close to my heart. Over the past few years, I’ve had the privilege of working as a Data Scientist and AI Engineer. Every single day, I’m reminded of how vast and dynamic the field of Artificial Intelligence (AI), Machine Learning (ML), Data Science, GenAI, NLP, MLOps, LLMOps etc is.</p>

<p>These fields have the power to transform industries, unlock creativity, and solve some of the world’s biggest challenges. But they can also feel overwhelming — especially if you’re just starting. That’s why I decided to create a space where we can explore this exciting world together.</p>

<h2 id="introducing-my-youtube-channel">Introducing My YouTube Channel</h2>

<p>After years of learning, experimenting, and growing, I’ve launched a <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">YouTube channel</a></strong> dedicated to AI, ML, Data Science, Computer Vision, Data Science, GenAI, NLP, MLOps, LLMOps etc . My goal is simple: to make these topics more accessible, less intimidating, and more actionable for anyone who’s curious about them.</p>

<p>Here’s what you can expect to find on the channel:</p>

<ul>
  <li>
    <p><strong>Breaking Down Complex Concepts</strong>: AI and ML can seem daunting, but they don’t have to be. I’ll simplify topics like neural networks, transformers, and Generative AI to make them easier to understand.</p>
  </li>
  <li>
    <p><strong>Hands-On Learning</strong>: Learning is about doing, so I’ll walk you through tutorials and projects. Whether it’s training a model or exploring MLOps, you’ll gain practical skills you can apply right away.</p>
  </li>
  <li>
    <p><strong>Book Recommendations and Reviews</strong>: There are so many incredible books in AI, Data Science, and beyond. I’ll share honest reviews and key takeaways to help you decide which ones are worth your time.</p>
  </li>
  <li>
    <p><strong>Conversations with Experts</strong>: I’ll be hosting thoughtful discussions with brilliant minds in AI, ML, Computer Vision, Data Science, GenAI, NLP, MLOps, LLMOps etc so we can all learn from their journeys and insights.</p>
  </li>
</ul>

<h2 id="why-this-matters-to-me">Why This Matters to Me</h2>

<p>When I first started in this field, I often felt lost. The technical jargon, the steep learning curves — it was a lot to take in. But I was fortunate to find resources and communities that made learning feel approachable and even fun.</p>

<p>Now, I want to give back by creating a similar space for others. Whether you’re a student, a professional looking to pivot into AI, or just someone curious about the field, I hope this channel becomes a resource you can trust and enjoy.</p>

<h2 id="learning-together">Learning Together</h2>

<p>One thing I’ve learned is that AI is as much about the process as it is about the results. It’s okay to start small, ask questions, and make mistakes. Every bit of progress, no matter how small, is worth celebrating.</p>

<p>This channel isn’t just about teaching; it’s about building a community of learners. Together, we can explore the latest advancements, tackle challenges, and uncover the limitless possibilities AI has to offer.</p>

<h2 id="join-me-on-this-journey">Join Me on This Journey</h2>

<p>If this resonates with you, I invite you to watch the channel trailer, subscribe, and be part of this adventure. Whether you’re new to AI or already experienced, there’s always more to learn, and I’d love for us to learn together.</p>

<p>Thank you for taking the time to read this. I’m excited about what lies ahead and grateful to have you as part of this journey. Here’s to exploring, learning, and growing together!</p>

<h2 id="connect-with-me">Connect with Me</h2>

<p>If you have any inquiries, feel free to reach out via message or email.</p>
<blockquote>
  <p><strong><a href="https://abonia1.github.io/">Website/Newletter</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Connect with me on <a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Find me on <a href="https://github.com/Abonia1">Github</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Visit my technical channel on <a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>
</blockquote>

<blockquote>
  <h1 id="thanks-for-reading"><em>Thanks for Reading!</em></h1>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Join me on an exciting journey exploring the world of Artificial Intelligence, Machine Learning, and Data Science. Whether you're just starting or have some experience, let's break down complex topics and learn together. My mission is to make AI and ML approachable and practical for everyone.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cdn-images-1.medium.com/max/2560/1*UbGdFVOr9zIZV5wkpcaSzg.png" /><media:content medium="image" url="https://cdn-images-1.medium.com/max/2560/1*UbGdFVOr9zIZV5wkpcaSzg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Automate ML and LLM Workflow with GitHub Actions &amp;amp; CML</title><link href="http://localhost:4000/blogs/2024-12-02-Deploy-and-Automate-ML-Workflow-GitHub-Actions-and-CML-for-CICD/" rel="alternate" type="text/html" title="Automate ML and LLM Workflow with GitHub Actions &amp;amp; CML" /><published>2024-12-02T00:00:00+01:00</published><updated>2024-11-29T13:32:40+01:00</updated><id>http://localhost:4000/blogs/Deploy-and-Automate-ML-Workflow-GitHub-Actions-and-CML-for-CICD</id><content type="html" xml:base="http://localhost:4000/blogs/2024-12-02-Deploy-and-Automate-ML-Workflow-GitHub-Actions-and-CML-for-CICD/"><![CDATA[<p><strong>Reading_time:</strong> 10 min<br />
  <strong>Tags:</strong> [GitHubActions, CML, MachineLearning, MLOps, DataScience, CICD, Automation, MLPipeline, AI, Scikitpipeline, MLModel
MachineLearning, GitHubActions, CML, MLAutomation, ChurnPrediction]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#automate-ml-and-llm-workflow-with-github-actions--cml" id="markdown-toc-automate-ml-and-llm-workflow-with-github-actions--cml">Automate ML and LLM Workflow with GitHub Actions &amp; CML</a>    <ul>
      <li><a href="#a-complete-guide-to-cicd-for-machine-learning-or-llm-projects" id="markdown-toc-a-complete-guide-to-cicd-for-machine-learning-or-llm-projects">A Complete Guide to CI/CD for Machine Learning or LLM Projects</a></li>
      <li><a href="#what-is-cicd-for-machine-learning" id="markdown-toc-what-is-cicd-for-machine-learning">What is CI/CD for Machine Learning?</a></li>
    </ul>
  </li>
  <li><a href="#part-1-automating-ml-workflow-with-github-actions--cml" id="markdown-toc-part-1-automating-ml-workflow-with-github-actions--cml">Part 1: Automating ML Workflow with GitHub Actions &amp; CML</a>    <ul>
      <li><a href="#step-by-step-tutorial-setting-up-cicd-for-a-churn-prediction-project" id="markdown-toc-step-by-step-tutorial-setting-up-cicd-for-a-churn-prediction-project">Step-by-Step Tutorial: Setting Up CI/CD for a Churn Prediction Project</a></li>
    </ul>
  </li>
  <li><a href="#part-2-enhancing-the-ml-workflow-with-github-actions--cml" id="markdown-toc-part-2-enhancing-the-ml-workflow-with-github-actions--cml">Part 2: Enhancing the ML Workflow with GitHub Actions &amp; CML</a></li>
  <li><a href="#what-is-github-actions" id="markdown-toc-what-is-github-actions">What is GitHub Actions?</a></li>
  <li><a href="#key-features-of-github-actions" id="markdown-toc-key-features-of-github-actions">Key Features of GitHub Actions:</a></li>
  <li><a href="#sample-github-actions-workflow-yaml" id="markdown-toc-sample-github-actions-workflow-yaml">Sample GitHub Actions Workflow YAML:</a></li>
  <li><a href="#what-is-cml-continuous-machine-learning" id="markdown-toc-what-is-cml-continuous-machine-learning">What is CML (Continuous Machine Learning)?</a></li>
  <li><a href="#key-features-of-cml" id="markdown-toc-key-features-of-cml">Key Features of CML:</a></li>
  <li><a href="#sample-cml-workflow-with-github-actions" id="markdown-toc-sample-cml-workflow-with-github-actions">Sample CML Workflow with GitHub Actions:</a></li>
  <li><a href="#explanation-of-the-workflow" id="markdown-toc-explanation-of-the-workflow">Explanation of the Workflow:</a>    <ul>
      <li><a href="#key-benefits-of-automating-ml-workflows-with-github-actions--cml" id="markdown-toc-key-benefits-of-automating-ml-workflows-with-github-actions--cml">Key Benefits of Automating ML Workflows with GitHub Actions &amp; CML</a></li>
      <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
    </ul>
  </li>
  <li><a href="#thanks-for-reading" id="markdown-toc-thanks-for-reading"><em>Thanks for Reading!</em></a></li>
</ul>

<h2 id="automate-ml-and-llm-workflow-with-github-actions--cml">Automate ML and LLM Workflow with GitHub Actions &amp; CML</h2>

<h3 id="a-complete-guide-to-cicd-for-machine-learning-or-llm-projects">A Complete Guide to CI/CD for Machine Learning or LLM Projects</h3>

<p>Machine learning workflows are complex and time-consuming, involving tasks like data processing, model training, and evaluation. Integrating Continuous Integration (CI) and Continuous Deployment (CD) practices can automate these tasks, saving time, reducing errors, and improving collaboration.I have prepared a tutorial into two major parts as follow:</p>

<p><strong><a href="https://youtu.be/DQIX7p5xx7c">Part 1 -  MLOps On GitHub - Deploy and Automate ML Workflow - Using GitHub Actions and CML for CI &amp; CD</a></strong>.</p>

<p><strong><a href="https://youtu.be/u_rCPdZY2g4">Part 2: MLOps On GitHub - Deploy and Automate ML Workflow - Using GitHub Actions and CML for CI &amp; CD</a></strong>.</p>

<p>Here is the link to the Part1 tutorial on</p>

<p>In this comprehensive guide, we’ll explore how to automate ML workflow using GitHub Actions and Continuous Machine Learning (CML). We’ll focus on a churn prediction project and walk through the process of setting up an automated pipeline to handle training, evaluation, and reporting. By the end of this guide, you’ll have a fully automated CI/CD pipeline for ML project, enhancing workflow and making it easier to collaborate with team.</p>

<h3 id="what-is-cicd-for-machine-learning">What is CI/CD for Machine Learning?</h3>

<p>CI/CD is a set of practices that involve continuously integrating code changes and deploying them into production systems. For ML projects, CI/CD can automate various tasks such as model training, evaluation, testing, and deployment. The goal is to ensure that changes to the project — whether new data, code changes, or model updates — are automatically tested and deployed without manual intervention.</p>

<p>Continuous Integration (CI) focuses on automatically testing and integrating code changes into the main branch. For ML projects, this involves testing the model’s accuracy, performance, and behavior with each update.</p>

<p>Continuous Deployment (CD) automates the deployment of the model to a production environment. This ensures that the most recent version of the model is available for use, and any issues are detected quickly.</p>

<p>For ML, CI/CD practices help ensure that models are continuously trained, tested, and deployed without manual intervention, allowing for faster iteration and a smoother workflow.</p>

<h2 id="part-1-automating-ml-workflow-with-github-actions--cml">Part 1: Automating ML Workflow with GitHub Actions &amp; CML</h2>

<p><strong>Introduction to GitHub Actions and CML</strong></p>

<p>GitHub Actions is a tool that allows you to automate software workflows directly within GitHub repository. It is ideal for CI/CD purposes, where you can define workflows that automatically trigger when specific events occur (e.g., a code push, a pull request).</p>

<p>Continuous Machine Learning (CML) is an open-source tool designed to help you incorporate continuous integration and delivery into ML projects. CML integrates seamlessly with GitHub Actions, allowing you to automate tasks such as model training, evaluation, and report generation.</p>

<h3 id="step-by-step-tutorial-setting-up-cicd-for-a-churn-prediction-project">Step-by-Step Tutorial: Setting Up CI/CD for a Churn Prediction Project</h3>

<p><strong>1. Setting Up the GitHub Repository</strong></p>

<p>To get started, you’ll need to create a GitHub repository for churn prediction project. This repository will store code, datasets, and results. You’ll also configure GitHub Actions to automate the ML pipeline.</p>

<ul>
  <li>
    <p>Create a GitHub repository for project.</p>
  </li>
  <li>
    <p>Add a basic folder structure to organize code, data, and models.</p>
  </li>
  <li>
    <p>Configure GitHub Actions in the repository by adding a workflow file (<code class="language-plaintext highlighter-rouge">.github/workflows</code>).</p>
  </li>
</ul>

<p><strong>2. Dataset Preparation</strong></p>

<p>The next step is preparing dataset. For this tutorial, we’ll use a churn prediction dataset. Since machine learning datasets can be large, Git LFS (Large File Storage) is essential for managing large files such as datasets and model weights.</p>

<ul>
  <li>
    <p>Upload the dataset to GitHub repository using Git LFS.</p>
  </li>
  <li>
    <p>If dataset is hosted elsewhere (e.g., cloud storage), make sure to automate the process of downloading the dataset within the workflow file.</p>
  </li>
</ul>

<p><strong>3. Building the ML Model Pipeline</strong></p>

<p>Here, we’ll build the machine learning pipeline for churn prediction. This includes tasks like data preprocessing, model training, and evaluation. With GitHub Actions, every time a change is pushed to the repository, the pipeline will automatically run to train the model and generate evaluation metrics.</p>

<ul>
  <li>
    <p>Create Python scripts for data preprocessing, model training, and testing.</p>
  </li>
  <li>
    <p>Define the machine learning algorithm (e.g., logistic regression, decision trees) for churn prediction.</p>
  </li>
</ul>

<p><strong>4. Testing the Model</strong></p>

<p>Once the model is trained, we’ll evaluate its performance. Using GitHub Actions and CML, we can automate the testing of the model’s accuracy and other metrics (e.g., confusion matrix, ROC curve). This ensures that the model’s performance is continuously monitored and automatically updated with every commit.</p>

<ul>
  <li>
    <p>Use CML to generate detailed performance reports.</p>
  </li>
  <li>
    <p>Set up automated testing for every commit to ensure the model is performing as expected.</p>
  </li>
</ul>

<p><strong>5. Next Steps</strong></p>

<p>After completing Part 1, you’ll have a fully automated pipeline for training and testing churn prediction model. In the next part, we’ll enhance the pipeline with visualizations, advanced metrics, and automated reports.</p>

<h2 id="part-2-enhancing-the-ml-workflow-with-github-actions--cml">Part 2: Enhancing the ML Workflow with GitHub Actions &amp; CML</h2>

<p><strong>1. Reviewing Model Evaluation Metrics</strong></p>

<p>In this part of the tutorial, we’ll focus on enhancing our CI/CD pipeline by adding more advanced model evaluation metrics, such as confusion matrix and accuracy graphs. By integrating these into the workflow, we can automatically assess model performance after every update.</p>

<ul>
  <li>Review the confusion matrix to understand how well our model is performing in terms of true positives, false positives, true negatives, and false negatives.</li>
</ul>

<p><strong>2. Git LFS Setup</strong></p>

<p>Git LFS (Large File Storage) ensures that large files like datasets and model weights are efficiently handled in our GitHub repository. It’s important to revisit Git LFS setup to ensure that everything works smoothly when training models on large datasets.</p>

<ul>
  <li>Install and configure Git LFS to handle large files in our project repository.</li>
</ul>

<p><strong>3. Creating the Workflow File (YAML Workflow Steps)</strong></p>

<p>The heart of our automation is the YAML workflow file. In this file, you’ll define the steps for training the model, testing it, and generating reports. The workflow file also specifies when each task should run (e.g., on every push or pull request).</p>

<ul>
  <li>
    <p>Define jobs in the workflow file for each step: data preparation, training, testing, and reporting.</p>
  </li>
  <li>
    <p>Automate the process of checking the model’s performance with every commit or pull request.</p>
  </li>
</ul>

<p><strong>4. Building the CI/CD Pipeline Job</strong></p>

<p>Now that the workflow file is set up, we’ll define the build job to automate the entire pipeline. The job will pull the latest code, prepare the environment, and run the necessary tasks (such as model training and testing) automatically.</p>

<ul>
  <li>Automate testing to ensure that every commit passes the required checks before being merged into the main branch.</li>
</ul>

<p><strong>5. Model Metrics and Visualizations with CML</strong></p>

<p>In this step, we’ll integrate CML to automatically generate performance visualizations (e.g., training accuracy, loss graphs) and display them directly within GitHub. These visualizations provide insights into model performance and can help track progress over time.</p>

<ul>
  <li>
    <p>Use CML to generate visualizations and save them as images or reports.</p>
  </li>
  <li>
    <p>Automatically upload and display these reports on GitHub.</p>
  </li>
</ul>

<p><strong>6. Automated Reports in Pull Requests</strong></p>

<p>Once the model is trained and tested, CML can be used to automatically generate a detailed report that is included in the pull request. This allows our team to review model performance directly from the GitHub interface without manually reviewing logs or results.</p>

<ul>
  <li>Set up CML to generate and attach reports in pull requests, making it easy for our team to monitor changes in model performance.</li>
</ul>

<p><strong>7. Conclusion and Next Steps</strong></p>

<p>By the end of Part 2, you’ll have a robust CI/CD pipeline that automates not only the training and testing of models but also the reporting of metrics and visualizations. This enables our team to track changes in model performance automatically and collaborate seamlessly on improvements.</p>

<p>Moving forward, you can scale this pipeline, integrate deployment workflows, and continuously improve the churn prediction model.</p>

<h2 id="what-is-github-actions">What is GitHub Actions?</h2>

<p><strong>GitHub Actions</strong> is a powerful tool provided by GitHub that allows you to automate workflows directly in our GitHub repository. It enables to define custom workflows for software development processes, such as Continuous Integration (CI), Continuous Deployment (CD), and automation of various tasks like testing, building, and deployment.</p>

<p>With GitHub Actions, you can automate virtually any task, and the workflow can be triggered by different events such as pushing code to a repository, opening a pull request, creating a tag, or on a scheduled basis. GitHub Actions use <strong>YAML configuration files</strong> to define workflows and steps.</p>

<h2 id="key-features-of-github-actions">Key Features of GitHub Actions:</h2>

<ul>
  <li>
    <p><strong>Custom Workflows</strong>: Create workflows that automatically trigger when events happen in repository.</p>
  </li>
  <li>
    <p><strong>Reusable Actions</strong>: You can reuse pre-built actions from the GitHub marketplace or create our own.</p>
  </li>
  <li>
    <p><strong>Parallel Execution</strong>: Workflows can run multiple jobs in parallel, improving efficiency.</p>
  </li>
  <li>
    <p><strong>Integration with Other Services</strong>: GitHub Actions can integrate with external services for additional tasks like sending notifications or deploying models to cloud platforms.</p>
  </li>
</ul>

<h2 id="sample-github-actions-workflow-yaml">Sample GitHub Actions Workflow YAML:</h2>

<p>Here’s a simple example of a GitHub Actions workflow file that runs tests on every push to the repository.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>name: Python CI
on:
  push:
    branches:
      - main  # Trigger this workflow on push to the main branch
  pull_request:
    branches:
      - main  # Trigger on pull requests to the main branch
jobs:
  test:
    runs-on: ubuntu-latest  # The machine that runs the workflow
    steps:
      - name: Checkout code
        uses: actions/checkout@v3  # This step checks out the repository
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'  # Set up Python 3.8 environment
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt  # Install dependencies from the requirements.txt
      - name: Run tests
        run: |
          pytest  # Run tests with pytest
</code></pre></div></div>

<h2 id="what-is-cml-continuous-machine-learning">What is CML (Continuous Machine Learning)?</h2>

<p><strong>Continuous Machine Learning (CML)</strong> is an open-source framework built by <strong>Iterative</strong> to bring continuous integration practices to machine learning (ML) workflows. CML allows you to automate aspects of machine learning lifecycle, such as training models, evaluating them, and generating reports, and integrates seamlessly with GitHub Actions.</p>

<p>CML makes it easy to manage the often complex and resource-heavy ML workflows by automating tasks such as:</p>

<ul>
  <li>
    <p>Training machine learning models</p>
  </li>
  <li>
    <p>Generating visualizations and metrics</p>
  </li>
  <li>
    <p>Creating automated reports</p>
  </li>
  <li>
    <p>Storing and versioning large datasets</p>
  </li>
</ul>

<p>With CML, you can ensure that models are constantly updated, evaluated, and tracked in an efficient, automated manner. It integrates well with GitHub Actions to enhance automation in ML projects.</p>

<h2 id="key-features-of-cml">Key Features of CML:</h2>

<ul>
  <li>
    <p><strong>Track Metrics and Visualizations</strong>: Automate the generation of charts, graphs, and other visualizations for model performance.</p>
  </li>
  <li>
    <p><strong>Model Experimentation</strong>: Automatically manage and track different versions of machine learning models.</p>
  </li>
  <li>
    <p><strong>Run and Report Model Evaluations</strong>: After training a model, automatically evaluate its performance and report metrics like accuracy, loss, confusion matrices, and more.</p>
  </li>
  <li>
    <p><strong>Integration with GitHub</strong>: CML integrates directly into GitHub Actions workflows, allowing for seamless automation of model training, testing, and reporting.</p>
  </li>
</ul>

<h2 id="sample-cml-workflow-with-github-actions">Sample CML Workflow with GitHub Actions:</h2>

<p>Below is an example of how you can integrate <strong>CML</strong> with <strong>GitHub Actions</strong> to automate ML workflows, including training a model, evaluating it, and generating a performance report.</p>

<p><strong>Define the CML Workflow in GitHub Actions</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>name: ML CI/CD Workflow
on:
  push:
    branches:
      - main  # Trigger workflow on push to main branch
  pull_request:
    branches:
      - main  # Trigger workflow for pull requests to the main branch
jobs:
  train:
    runs-on: ubuntu-latest  # Set the environment
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3  # Checkout code repository
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'  # Define the Python version
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt  # Install Python dependencies
      - name: Set up Git LFS
        run: |
          git lfs install  # Enable Git LFS to manage large files
      - name: Train model and log metrics with CML
        run: |
          python train_model.py  # Train model 
          cml run -T "Training Results" --report metrics.json  # Run CML to log metrics
      - name: Commit metrics and model results
        run: |
          git add metrics.json model.pkl  # Add metrics and model to Git
          git commit -m "Update model and metrics"
          git push
</code></pre></div></div>

<h2 id="explanation-of-the-workflow">Explanation of the Workflow:</h2>

<ul>
  <li>
    <p><strong>on:</strong>: Defines the trigger for the workflow. Here, the workflow runs on push to the main branch or when a pull request is opened.</p>
  </li>
  <li>
    <p><strong>jobs:</strong>: Defines the steps that run in the workflow. In this case, it includes:</p>
  </li>
</ul>

<ol>
  <li>
    <p><strong>Checkout code</strong>: It checks out repository’s code.</p>
  </li>
  <li>
    <p><strong>Set up Python</strong>: It sets up the Python environment for the workflow.</p>
  </li>
  <li>
    <p><strong>Install dependencies</strong>: It installs the necessary Python packages listed in the requirements.txt file.</p>
  </li>
  <li>
    <p><strong>Train the model</strong>: It runs the train_model.py script to train the model.</p>
  </li>
  <li>
    <p><strong>CML integration</strong>: The cml run command trains the model and logs metrics in metrics.json. The metrics and results are then committed back to the repository.</p>
  </li>
  <li>
    <p><strong>Push results</strong>: After the model is trained and metrics are generated, the results are committed and pushed to the repository.</p>
  </li>
</ol>

<h3 id="key-benefits-of-automating-ml-workflows-with-github-actions--cml">Key Benefits of Automating ML Workflows with GitHub Actions &amp; CML</h3>

<ol>
  <li>
    <p>Improved Efficiency: Automating repetitive tasks like model training, testing, and reporting saves valuable time and reduces the chances of human error.</p>
  </li>
  <li>
    <p>Real-Time Feedback: Automated reporting provides real-time insights into model performance, allowing team to quickly identify issues and improve the model.</p>
  </li>
  <li>
    <p>Collaborative Workflow: With GitHub Actions and CML, collaboration becomes seamless. Team members can contribute to the project, and every change is automatically tested and reviewed.</p>
  </li>
  <li>
    <p>Scalability: As project grows, you can easily scale the pipeline to handle more models, larger datasets, and more complex workflows.</p>
  </li>
</ol>

<h3 id="conclusion">Conclusion</h3>

<p>Automating machine learning workflow with GitHub Actions and CML is a game-changer for data scientists, machine learning engineers, and developers. By following this guide, you’ll have a fully automated CI/CD pipeline that handles training, evaluation, and reporting for churn prediction model.</p>

<p>Incorporating CI/CD into ML workflow ensures that team can work more efficiently, reduce errors, and stay up to date with the latest model improvements. So, what are you waiting for? Start automating ML projects today and enjoy the benefits of streamlined workflows and better collaboration. Happy coding!</p>

<blockquote>
  <p><strong>Connect with me on <a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Find me on <a href="https://github.com/Abonia1">Github</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Visit my technical channel on <a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Support: <a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>
</blockquote>

<blockquote>
  <h1 id="thanks-for-reading"><em>Thanks for Reading!</em></h1>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[MLOps On GitHub | Deploy and Automate ML Workflow |Using GitHub Actions and CML for CI & CD. Machine learning workflows are complex and time-consuming, involving tasks like data processing, model training, and evaluation. Integrating Continuous Integration (CI) and Continuous Deployment (CD) practices can automate these tasks, saving time, reducing errors, and improving collaboration.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cdn-images-1.medium.com/max/2560/1*y0qMmXhAevudLX23NgvkuA.png" /><media:content medium="image" url="https://cdn-images-1.medium.com/max/2560/1*y0qMmXhAevudLX23NgvkuA.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SAM 2 Advanced Object Segmentation for Images and Videos</title><link href="http://localhost:4000/blogs/2024-10-05-SAM2-Fine-Tuning-Custom-Dataset/" rel="alternate" type="text/html" title="SAM 2 Advanced Object Segmentation for Images and Videos" /><published>2024-10-05T00:00:00+02:00</published><updated>2024-09-25T15:43:26+02:00</updated><id>http://localhost:4000/blogs/SAM2-Fine-Tuning-Custom-Dataset</id><content type="html" xml:base="http://localhost:4000/blogs/2024-10-05-SAM2-Fine-Tuning-Custom-Dataset/"><![CDATA[<p><strong><a href="https://youtu.be/93xJstG_zl8">Link to the complete hands-on tutorial on Advanced Object Segmentation for Images</a></strong></p>

<p><strong>Reading_time:</strong> 5 min<br />
  <strong>Tags:</strong> [SAM, Object Segmentation, Segment Anything, Object Detection, Tracking , ComputerVision]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#object-segmentation--segment-anything" id="markdown-toc-object-segmentation--segment-anything">Object Segmentation — Segment Anything</a></li>
  <li><a href="#key-features" id="markdown-toc-key-features">Key Features</a>    <ul>
      <li><a href="#unified-model-architecture" id="markdown-toc-unified-model-architecture">Unified Model Architecture</a></li>
      <li><a href="#real-time-performance" id="markdown-toc-real-time-performance">Real-Time Performance</a></li>
      <li><a href="#zero-shot-generalization" id="markdown-toc-zero-shot-generalization">Zero-Shot Generalization</a></li>
      <li><a href="#interactive-refinement" id="markdown-toc-interactive-refinement">Interactive Refinement</a></li>
      <li><a href="#advanced-visual-handling" id="markdown-toc-advanced-visual-handling">Advanced Visual Handling</a></li>
    </ul>
  </li>
  <li><a href="#getting-started-with-sam-2" id="markdown-toc-getting-started-with-sam-2">Getting Started with SAM 2</a>    <ul>
      <li><a href="#complete-notebook" id="markdown-toc-complete-notebook">Complete Notebook</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>

<h3 id="object-segmentation--segment-anything">Object Segmentation — Segment Anything</h3>

<p>SAM 2 (Segment Anything Model 2) is an advanced machine learning model designed for comprehensive object segmentation in both static images and dynamic videos. Developed by Meta AI Research, SAM 2 represents a significant leap forward in computer vision capabilities, offering real-time performance and zero-shot generalization. This tutorial will explore the key features of SAM 2, its architecture, and how to get started with using this powerful tool.</p>

<iframe width="900" height="500" src="https://www.youtube.com/embed/93xJstG_zl8" frameborder="0" allowfullscreen=""></iframe>

<h2 id="key-features">Key Features</h2>

<h3 id="unified-model-architecture">Unified Model Architecture</h3>

<p>SAM 2 boasts a unified model architecture that can handle both image and video segmentation tasks efficiently. This design allows for seamless integration across various applications, from still-image analysis to real-time video processing.</p>

<h3 id="real-time-performance">Real-Time Performance</h3>

<p>One of the standout features of SAM 2 is its ability to operate in real-time, making it suitable for applications requiring immediate visual analysis. This capability is crucial for applications such as autonomous vehicles, surveillance systems, and interactive media platforms.</p>

<h3 id="zero-shot-generalization">Zero-Shot Generalization</h3>

<p>Unlike traditional models that require extensive fine-tuning for new tasks, SAM 2 demonstrates zero-shot generalization. This means it can adapt to new object categories without retraining, significantly reducing the time and computational resources required for deployment in new scenarios .</p>

<h3 id="interactive-refinement">Interactive Refinement</h3>

<p>SAM 2 incorporates an interactive refinement feature, allowing users to guide the segmentation process through simple text prompts. This capability enhances the model’s flexibility and makes it easier to refine segmentations based on specific requirements .</p>

<h3 id="advanced-visual-handling">Advanced Visual Handling</h3>

<p>The model is equipped with advanced visual handling capabilities, enabling it to process complex visual data effectively. This feature is particularly useful for dealing with challenging scenes, occlusions, and varying lighting conditions .</p>

<h2 id="getting-started-with-sam-2">Getting Started with SAM 2</h2>

<p>To begin working with SAM 2, follow these steps:</p>
<blockquote>
  <p>📌 Github : <a href="https://github.com/facebookresearch/segment-anything-2">https://github.com/facebookresearch/segment-anything-2</a>
 📌 Paper: <a href="https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/">https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/</a>
 📌 Demo: <a href="https://sam2.metademolab.com/">https://sam2.metademolab.com/</a>
 📌 Dataset: <a href="https://ai.meta.com/datasets/segment-anything-video/">https://ai.meta.com/datasets/segment-anything-video/</a></p>
</blockquote>

<ol>
  <li>
    <p>Download and Installation: In the tutorial, we have download SAM 2 from its official repo . Ensure you have the required dependencies installed on your system.</p>

    <p>!git clone https://github.com/facebookresearch/segment-anything-2.git</p>

    <p>!pip install -e /content/segment-anything-2</p>
  </li>
</ol>

<p>In colab , to download checkpoints,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Change to the checkpoints directory
%cd /content/segment-anything-2/checkpoints

# Make sure the script is executable
!chmod +x download_ckpts.sh

# Run the script
!./download_ckpts.sh
</code></pre></div></div>

<ol>
  <li>
    <p>Model Loading: Load the pre-trained SAM 2 model into your preferred deep learning framework. The model weights are typically stored in a PyTorch-compatible format.</p>

    <p>import numpy as np
 import torch
 import matplotlib.pyplot as plt
 from PIL import Image</p>

    <p>#use bfloat16 for the entire notebook
 torch.autocast(device_type=”cuda”, dtype=torch.bfloat16).<strong>enter</strong>()</p>

    <p>if torch.cuda.get_device_properties(0).major &gt;= 8:
 #turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)
 torch.backends.cuda.matmul.allow_tf32 = True
 torch.backends.cudnn.allow_tf32 = True</p>

    <p>from sam2.build_sam import build_sam2
 from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator</p>

    <p>sam2_checkpoint = “../checkpoints/sam2_hiera_large.pt”
 model_cfg = “sam2_hiera_l.yaml”</p>

    <p>sam2 = build_sam2(model_cfg, sam2_checkpoint, device =’cuda’, apply_postprocessing=False)</p>

    <p>mask_generator = SAM2AutomaticMaskGenerator(sam2)</p>
  </li>
  <li>
    <p>Image/Video Input: Prepare your input images or videos. SAM 2 can handle both single-frame images and video streams.</p>
  </li>
  <li>
    <p>Segmentation Process: Apply the loaded model to generate masks for objects within the input images or frames.</p>
  </li>
  <li>
    <p>Post-processing: Refine the generated masks if needed, utilizing the interactive refinement capabilities of SAM 2.</p>
  </li>
</ol>

<p>Here is the code sample :</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import os

def generate_mask_plot(image, mask_generator):
  image = Image.open(image)
  image = np.array(image.convert("RGB"))
  masks = mask_generator.generate(image)
  plt.figure(figsize=(20,20))
  plt.imshow(image)
  show_annotations(masks)
  plt.axis('off')
  plt.show()

def show_annotations(anns, borders=True):
    if len(anns) == 0:
        return
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)

    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:,:,3] = 0
    for ann in sorted_anns:
        m = ann['segmentation']
        color_mask = np.concatenate([np.random.random(3), [0.5]])
        img[m] = color_mask
        if borders:
            import cv2
            contours, _ = cv2.findContours(m.astype(np.uint8),cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
            # Try to smooth contours
            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]
            cv2.drawContours(img, contours, -1, (0,0,1,0.4), thickness=1)

    ax.imshow(img)

image_dir = '/content/images/'
image_names = [f for f in os.listdir(image_dir) if f.endswith(('.jpeg', '.png', '.jpg'))]
for image_name in image_names:
  generate_mask_plot(image_dir+image_name, mask_generator)
</code></pre></div></div>

<h3 id="complete-notebook">Complete Notebook</h3>

<script src="https://gist.github.com/Abonia1/ee907b4dbd630b38dd088dcee2f84188.js"></script>

<h2 id="conclusion">Conclusion</h2>

<p>SAM 2 represents a significant advancement in object segmentation technology, offering real-time performance, zero-shot generalization, and advanced visual handling capabilities. Its unified architecture makes it versatile across various applications, from static image analysis to dynamic video processing. As researchers continue to refine and expand SAM 2’s capabilities, we can expect to see even more innovative applications in fields such as computer vision, robotics, and interactive media.</p>

<hr />
<p><strong><em>Thanks for Reading!</em></strong></p>

<p><strong><a href="https://abonia1.github.io/">Website/Newletter</a></strong>
<strong><a href="https://aboniasojasingarayar.substack.com">AIMagazine Substack</a></strong></p>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[SAM 2 (Segment Anything Model 2) is an advanced machine learning model designed for comprehensive object segmentation in both static images and dynamic videos. Developed by Meta AI Research, SAM 2 represents a significant leap forward in computer vision capabilities, offering real-time performance and zero-shot generalization. This tutorial will explore the key features of SAM 2, its architecture, and how to get started with using this powerful tool.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/SAM2.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/SAM2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multi-Modal Retrieval - Bridging Text and Images with BGE and CLIP</title><link href="http://localhost:4000/blogs/2024-10-01-Multi-Modal-Retrieval-BGE-CLIP/" rel="alternate" type="text/html" title="Multi-Modal Retrieval - Bridging Text and Images with BGE and CLIP" /><published>2024-10-01T00:00:00+02:00</published><updated>2024-09-25T15:43:26+02:00</updated><id>http://localhost:4000/blogs/Multi-Modal-Retrieval-BGE-CLIP</id><content type="html" xml:base="http://localhost:4000/blogs/2024-10-01-Multi-Modal-Retrieval-BGE-CLIP/"><![CDATA[<p><strong><a href="https://youtu.be/9oogZrSTiM0?si=epr5kNKcrbzeZ52z">Link to the complete hands-on tutorial on Multi-Modal Retrieval - Bridging Text and Images with BGE and CLIP</a></strong></p>

<p><strong>Reading_time:</strong> 5 min<br />
  <strong>Tags:</strong> [MultimodalRetrieval, AI, MachineLearning, NLP, ComputerVision, CLIP, BGE, ArtificialIntelligence, CrossModalSearch, TextEmbedding, ImageEmbedding, VectorSearch]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#multi-modal-retrieval-bridging-text-and-images-with-bge-and-clip" id="markdown-toc-multi-modal-retrieval-bridging-text-and-images-with-bge-and-clip">Multi-Modal Retrieval: Bridging Text and Images with BGE and CLIP</a>    <ul>
      <li><a href="#building-and-testing-a-multi-modal-retriever--hands-on" id="markdown-toc-building-and-testing-a-multi-modal-retriever--hands-on">Building and Testing a Multi-Modal Retriever — Hands-On</a></li>
    </ul>
  </li>
  <li><a href="#understanding-multi-modal-retrieval" id="markdown-toc-understanding-multi-modal-retrieval">Understanding Multi-Modal Retrieval</a>    <ul>
      <li><a href="#technical-overview" id="markdown-toc-technical-overview">Technical Overview</a></li>
      <li><a href="#data-preparation" id="markdown-toc-data-preparation">Data Preparation</a></li>
      <li><a href="#building-the-vector-store" id="markdown-toc-building-the-vector-store">Building the Vector Store</a></li>
      <li><a href="#query-processing-and-retrieval" id="markdown-toc-query-processing-and-retrieval">Query Processing and Retrieval</a></li>
    </ul>
  </li>
  <li><a href="#tutorial" id="markdown-toc-tutorial">Tutorial</a></li>
  <li><a href="#colab-notebook" id="markdown-toc-colab-notebook">Colab Notebook</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>

<h2 id="multi-modal-retrieval-bridging-text-and-images-with-bge-and-clip">Multi-Modal Retrieval: Bridging Text and Images with BGE and CLIP</h2>

<h3 id="building-and-testing-a-multi-modal-retriever--hands-on">Building and Testing a Multi-Modal Retriever — Hands-On</h3>

<p>In today’s data-rich world, being able to retrieve relevant information across different modalities (text, images, audio) has become increasingly important. This post will guide you through creating a multi-modal retrieval system that combines text embeddings from BGE (Bidirectional Generative Encoder) and image embeddings from CLIP (Contrastive Language-Instrumental Pre-training) to index and query Wikipedia articles.</p>

<iframe width="900" height="500" src="https://www.youtube.com/embed/9oogZrSTiM0" frameborder="0" allowfullscreen=""></iframe>

<p>In the rapidly evolving landscape of artificial intelligence and natural language processing, the ability to seamlessly integrate text and image information has become increasingly crucial. This article explores a cutting-edge approach to achieving this integration, leveraging state-of-the-art techniques to create a powerful demonstration of cross-modal retrieval capabilities.</p>

<h2 id="understanding-multi-modal-retrieval">Understanding Multi-Modal Retrieval</h2>

<p>Multi-modal retrieval allows us to search for information based on multiple types of input, such as text queries and image queries. It’s particularly useful for applications like visual search, content-based recommendation systems, and multimedia analysis.</p>

<p>Key aspects of multi-modal retrieval include:</p>

<ul>
  <li>
    <p>Indexing: Creating embeddings for both text and image data</p>
  </li>
  <li>
    <p>Querying: Processing user inputs and finding similar matches</p>
  </li>
  <li>
    <p>Ranking: Ordering results based on relevance</p>
  </li>
</ul>

<h3 id="technical-overview">Technical Overview</h3>

<p>The proposed system utilizes several advanced techniques to enable efficient and accurate retrieval of both textual and visual content:</p>

<ol>
  <li>
    <p>Text Embedding Index: 
 — Utilizes BAAI/bge-small-en-v1.5 for text representation
 — Employs Hugging Face (HF) embedding for query encoding</p>
  </li>
  <li>
    <p>Image Embedding Index:
 — Implements CLIP embeddings from sentence transformers
 — Leverages CLIP embedding for image query encoding</p>
  </li>
  <li>
    <p>Query Encoder:
 — Processes text queries using HF embedding
 — Processes image queries using CLIP embedding</p>
  </li>
  <li>
    <p>Framework:
 — Built on top of LlamaIndex for efficient vector storage and retrieval</p>
  </li>
</ol>

<h3 id="data-preparation">Data Preparation</h3>

<p>Before implementing the retrieval system, a crucial step involves preparing the necessary datasets:</p>

<ul>
  <li>Download raw text and image files for Wikipedia articles</li>
  <li>These files serve as the foundation for building comprehensive indexes</li>
</ul>

<h3 id="building-the-vector-store">Building the Vector Store</h3>

<p>With the data at hand, the next step is to construct the vector store:</p>

<ol>
  <li>
    <p>Text Index Construction:
 — Apply BAAI/bge-small-en-v1.5 embeddings to text data
 — Create a vector store using these embeddings</p>
  </li>
  <li>
    <p>Image Index Construction:
 — Generate CLIP embeddings for image data
 — Build a separate vector store for the image embeddings</p>
  </li>
</ol>

<h3 id="query-processing-and-retrieval">Query Processing and Retrieval</h3>

<p>Once the vector store is populated, the system can handle various types of queries:</p>

<ol>
  <li>
    <p>Text Queries:
 — Encode input text using HF embedding
 — Search the text index for relevant matches</p>
  </li>
  <li>
    <p>Image Queries:
 — Extract features from input images using CLIP embedding
 — Search the image index for visually similar content</p>
  </li>
  <li>
    <p>Cross-Modal Queries:
 — Combine text and image queries for simultaneous retrieval
 — Leverage the power of both indices to find relevant information</p>
  </li>
</ol>

<h2 id="tutorial">Tutorial</h2>

<p>A user searches for “Bat man” using a combination of text and image queries. The system would:</p>

<ol>
  <li>Process the text query using HF embedding</li>
  <li>Retrieve relevant Wikipedia articles from the text index</li>
  <li>Simultaneously process the image query (e.g., a photo of Mars, Batman)</li>
  <li>Find visually similar images from the image index</li>
  <li>Return a combined result set that includes both textually and visually relevant information</li>
</ol>

<p>This tutorial highlights the system’s ability to bridge the gap between linguistic and visual representations, providing users with a comprehensive understanding of complex topics through multimodal retrieval.</p>

<h2 id="colab-notebook">Colab Notebook</h2>

<p><strong><a href="https://github.com/Abonia1/Multi-Modal-Retriever/blob/main/multi_modal_retrieval.ipynb">Link to the complete Notebook</a></strong></p>

<script src="https://gist.github.com/Abonia1/a0c555f6e53bc271fe305d29983af645.js"></script>

<p><img src="https://cdn-images-1.medium.com/max/2360/1*xkAvz-IDGh9MVxcx7MBkXA.png" alt="" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>By integrating cutting-edge text and image embedding techniques within a unified framework, this system offers a powerful tool for information retrieval across modalities. Its potential applications extend beyond simple demonstrations, potentially revolutionizing how we interact with and understand complex information in various domains such as education, research, and entertainment. As AI technology continues to advance, such systems will play an increasingly important role in facilitating human-computer interaction and knowledge acquisition.</p>

<p>Lets learn and grow together!</p>

<hr />
<p><strong><em>Thanks for Reading!</em></strong></p>

<p><strong><a href="https://abonia1.github.io/">Website/Newletter</a></strong>
<strong><a href="https://aboniasojasingarayar.substack.com">AIMagazine Substack</a></strong></p>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[In today’s data-rich world, being able to retrieve relevant information across different modalities (text, images, audio) has become increasingly important. This post will guide you through creating a multi-modal retrieval system that combines text embeddings from BGE (Bidirectional Generative Encoder) and image embeddings from CLIP (Contrastive Language-Instrumental Pre-training) to index and query Wikipedia articles.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/Multi-ModalRetriever.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/Multi-ModalRetriever.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Running Ollama in Google Colab (Free Tier)</title><link href="http://localhost:4000/blogs/2024-09-20-Running-Ollama-in-Google-Colab-(FreeTier)/" rel="alternate" type="text/html" title="Running Ollama in Google Colab (Free Tier)" /><published>2024-09-20T00:00:00+02:00</published><updated>2024-09-20T13:56:27+02:00</updated><id>http://localhost:4000/blogs/Running-Ollama-in-Google-Colab-(FreeTier)</id><content type="html" xml:base="http://localhost:4000/blogs/2024-09-20-Running-Ollama-in-Google-Colab-(FreeTier)/"><![CDATA[<p><strong><a href="https://youtu.be/UIZ5kdjYNMA">Link to the complete hands-on tutorial on Running Ollama in Google Colab (Free Tier)</a></strong></p>

<p><strong>Reading_time:</strong> 5 min<br />
  <strong>Tags:</strong> [Ollama, colab, FreeTier, LLM, GenAI]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#a-step-by-step-tutorial" id="markdown-toc-a-step-by-step-tutorial">A Step-by-Step Tutorial</a></li>
  <li><a href="#setting-up-the-environment" id="markdown-toc-setting-up-the-environment">Setting Up the Environment</a></li>
  <li><a href="#installing-and-serving-ollama" id="markdown-toc-installing-and-serving-ollama">Installing and serving Ollama</a>    <ul>
      <li><a href="#launching-xterm" id="markdown-toc-launching-xterm">Launching Xterm</a></li>
      <li><a href="#installing-ollama" id="markdown-toc-installing-ollama">Installing Ollama</a></li>
      <li><a href="#starting-the-ollama-server" id="markdown-toc-starting-the-ollama-server">Starting the Ollama Server</a></li>
      <li><a href="#pulling-ai-models" id="markdown-toc-pulling-ai-models">Pulling AI Models</a></li>
      <li><a href="#verifying-the-installation" id="markdown-toc-verifying-the-installation">Verifying the Installation</a></li>
    </ul>
  </li>
  <li><a href="#running-ollama-commands" id="markdown-toc-running-ollama-commands">Running Ollama Commands</a>    <ul>
      <li><a href="#sample-colab-notebook" id="markdown-toc-sample-colab-notebook">Sample Colab Notebook</a></li>
    </ul>
  </li>
  <li><a href="#key-points-to-consider" id="markdown-toc-key-points-to-consider">Key Points to Consider</a></li>
  <li><a href="#best-practices" id="markdown-toc-best-practices">Best Practices</a></li>
</ul>

<h3 id="a-step-by-step-tutorial">A Step-by-Step Tutorial</h3>

<p>Ollama empowers you to leverage powerful large language models (LLMs) like Llama2,Llama3,Phi3 etc. without needing a powerful local machine. Google Colab’s free tier provides a cloud environment perfectly suited for running these resource-intensive models. This tutorial details setting up and running Ollama on the free version of Google Colab, allowing you to explore the capabilities of LLMs without significant upfront costs.</p>

<iframe width="900" height="500" src="https://www.youtube.com/embed/UIZ5kdjYNMA" frameborder="0" allowfullscreen=""></iframe>

<p>You will learn:</p>

<ul>
  <li>
    <p>Why run Ollama in, Google Colab</p>
  </li>
  <li>
    <p>How to run Ollama in colab</p>
  </li>
</ul>

<p>Google Colab provides an excellent environment for running machine learning models and tools like Ollama. While Colab offers a generous free tier, we need to take some extra steps to ensure we can run Ollama effectively. Let’s go through this process step-by-step.</p>

<h2 id="setting-up-the-environment">Setting Up the Environment</h2>

<p>First, we need to set up our Colab notebook to support command-line operations:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!pip install colab-xterm
%load_ext colabxterm
</code></pre></div></div>

<p>This code installs the <code class="language-plaintext highlighter-rouge">colab-xterm</code> library and enables the Colab XTerm extension, which allows us to run shell commands directly in our notebook .</p>

<h2 id="installing-and-serving-ollama">Installing and serving Ollama</h2>

<p>To get started with Ollama, we’ll need to install it using the official installation script. Here’s how to do it:</p>

<h3 id="launching-xterm">Launching Xterm</h3>

<p>Now, let’s launch the xterm terminal within our Colab cell:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%xterm
</code></pre></div></div>

<p>This command opens a full-screen terminal window within your Colab notebook.</p>

<h3 id="installing-ollama">Installing Ollama</h3>

<p>Once the xterm is open, we can proceed with the installation of Ollama. Run the following commands:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl https://ollama.ai/install.sh | sh
</code></pre></div></div>

<p>This command downloads the installation script from the Ollama website and executes it. The script will handle the installation process automatically, including downloading and installing necessary dependencies.</p>

<h3 id="starting-the-ollama-server">Starting the Ollama Server</h3>

<p>Once Ollama is installed, we can start the server using the following command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama serve &amp;
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">&amp;</code> at the end runs the command in the background, allowing you to continue using your terminal.</p>

<h3 id="pulling-ai-models">Pulling AI Models</h3>

<p>Now that the Ollama server is running, we can pull AI models to use with our server. Let’s pull the Mistral model as an example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama pull mistral
</code></pre></div></div>

<p>This command downloads the Mistral model and makes it available for use with your Ollama server.</p>

<h3 id="verifying-the-installation">Verifying the Installation</h3>

<p>Let’s verify that Ollama has been installed correctly:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!ollama - version
</code></pre></div></div>

<p>This should display the version number of Ollama if the installation was successful.</p>

<h2 id="running-ollama-commands">Running Ollama Commands</h2>

<p>Now that we have Ollama installed, we can start using it. Here are a few basic commands to get you started:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!ollama pull llama
!ollama generate "Hello, world!"
</code></pre></div></div>

<p>The first command pulls the “llama” model, and the second generates text using that model.</p>

<h3 id="sample-colab-notebook">Sample Colab Notebook</h3>

<script src="https://gist.github.com/Abonia1/48ea45c147d15621b3f4dcc6a6ca2a2f.js"></script>

<h2 id="key-points-to-consider">Key Points to Consider</h2>

<ul>
  <li>While Ollama runs in Colab, it may not be as fast as running locally due to network latency and resource limitations of the free tier.</li>
  <li>Be mindful of your usage limits, especially if you plan to run multiple models or generate large amounts of text.</li>
  <li>Some advanced features of Ollama might not work perfectly in the Colab environment due to its virtual machine nature.</li>
</ul>

<h2 id="best-practices">Best Practices</h2>

<ol>
  <li>Use smaller models: Choose lighter models like “llama” or “llama2” for better performance in Colab.</li>
  <li>Generate text incrementally: If you need longer outputs, consider generating text in chunks rather than all at once.</li>
  <li>Save your work: Remember to save your notebook frequently, as Colab sessions can sometimes terminate unexpectedly.</li>
  <li>
    <p>Clean up: After your session ends, you might want to remove Ollama to free up space:</p>

    <p>!rm -rf /usr/local/bin/ollama</p>
  </li>
</ol>

<p>By following these steps and best practices, you can effectively run Ollama in Google Colab using the free tier. This setup provides a convenient way to experiment with language models without needing to manage a local server or worry about hardware requirements.</p>

<hr />
<p><strong><em>Thanks for Reading!</em></strong></p>

<p><strong><a href="https://abonia1.github.io/">Website/Newletter</a></strong>
<strong><a href="https://aboniasojasingarayar.substack.com">AIMagazine Substack</a></strong></p>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Ollama empowers you to leverage powerful large language models (LLMs) like Llama2,Llama3,Phi3 etc. without needing a powerful local machine. Google Colab’s free tier provides a cloud environment perfectly suited for running these resource-intensive models. This tutorial details setting up and running Ollama on the free version of Google Colab, allowing you to explore the capabilities of LLMs without significant upfront costs.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/ollama-in-colab.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/ollama-in-colab.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">2-Introduction to LLM — [AIMagazine]</title><link href="http://localhost:4000/blogs/2024-08-23-Introduction-to-LLM-AIMagazine/" rel="alternate" type="text/html" title="2-Introduction to LLM — [AIMagazine]" /><published>2024-08-23T00:00:00+02:00</published><updated>2024-09-20T13:36:27+02:00</updated><id>http://localhost:4000/blogs/Introduction-to-LLM-AIMagazine</id><content type="html" xml:base="http://localhost:4000/blogs/2024-08-23-Introduction-to-LLM-AIMagazine/"><![CDATA[<p><strong>Reading_time:</strong> 5 min<br />
  <strong>Tags:</strong> [LLM, GenAI, AI ,MachineLearning, NLP, ComputerVision, MLOps DeepLearning, DataScience, TechInnovation,FutureTech
]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#1-introduction-to-llm--aimagazine" id="markdown-toc-1-introduction-to-llm--aimagazine">1-Introduction to LLM — AIMagazine</a>    <ul>
      <li><a href="#dive-into-the-world-of-llm" id="markdown-toc-dive-into-the-world-of-llm">Dive into the World of LLM</a></li>
      <li><a href="#why-you-should-read-this" id="markdown-toc-why-you-should-read-this">Why You Should Read This</a></li>
      <li><a href="#stay-updated" id="markdown-toc-stay-updated">Stay Updated</a></li>
    </ul>
  </li>
  <li><a href="#read-more-on-aimagazine" id="markdown-toc-read-more-on-aimagazine"><strong>Read More on AIMagazine</strong></a></li>
</ul>

<h2 id="1-introduction-to-llm--aimagazine">1-Introduction to LLM — AIMagazine</h2>

<h3 id="dive-into-the-world-of-llm">Dive into the World of LLM</h3>

<p>Artificial intelligence, machine learning, natural language processing, (LLM), and MLOps are evolving at an incredible pace. Staying informed about the latest developments, research, and trends is crucial for anyone in the tech field. To help you keep up, I’ve published a detailed article on LLM on my Substack <strong><a href="https://aboniasojasingarayar.substack.com">AIMagazine</a></strong>.</p>

<p><img src="/assets/img/blog/LLM.png" alt="" /></p>

<p>Here’s a glimpse of what you can expect in this first chapter format article and what You’ll Learn:</p>

<ul>
  <li>
    <p>Definition and overview of LLMs</p>
  </li>
  <li>
    <p>Types of LLMs</p>
  </li>
  <li>
    <p>Key technical concepts</p>
  </li>
  <li>
    <p>Evaluating LLMs</p>
  </li>
  <li>
    <p>Applications, Challenges Limitation</p>
  </li>
  <li>
    <p>LLM Cheat Sheet</p>
  </li>
</ul>

<p>👉 <strong>Read the full <a href="https://open.substack.com/pub/aboniasojasingarayar/p/introduction-to-large-language-models?r=92g95&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true">article on Substack AIMagazine </a></strong></p>

<h3 id="why-you-should-read-this">Why You Should Read This</h3>

<p>This article aims to provide a foundational understanding of LLM and its applications. By the end of the article, you’ll be able to identify the different components of an LLM system and grasp the challenges involved in developing these systems. Whether you’re a beginner or someone looking to refresh your knowledge, this article is designed to offer valuable insights.</p>

<h3 id="stay-updated">Stay Updated</h3>

<p>I’m planning to write articles in a chapter format for an in-depth exploration of each topic, covering aspects like NLP, LLMs, fine-tuning, evaluation, RAG, information retrieval, deployment, knowledge graphs, and more. There are many other concepts to be added, and I will update them in subsequent posts.</p>

<h2 id="read-more-on-aimagazine"><strong>Read More on AIMagazine</strong></h2>

<p>To dive deeper into these topics, I invite you to read the full article on my substack <strong><a href="https://aboniasojasingarayar.substack.com">AIMagazine</a></strong>. Your feedback and engagement are highly valued, and subscribing will ensure you get updates directly in your inbox whenever new articles are published.</p>

<hr />
<p><strong><em>Thanks for Reading!</em></strong></p>

<p><strong><a href="https://abonia1.github.io/">Website/Newletter</a></strong></p>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[This chapter will be your comprehensive guide to navigating the fascinating world of LLMs. We'll delve into their core concepts, exploring different types like autoregressive models and encoder-decoder models. You'll discover the magic behind self-attention, a mechanism that allows LLMs to focus on relevant information, and delve into the pre-training strategies that give them their vast knowledge. Finally, we'll showcase the real-world applications of LLMs, from powering chatbots and generating realistic dialogue to creating marketing copy and summarizing complex topics.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/AIMagazine.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/AIMagazine.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">1-Introduction to NLP — [AIMagazine]</title><link href="http://localhost:4000/blogs/2024-07-02-Introduction-to-NLP-AIMagazine/" rel="alternate" type="text/html" title="1-Introduction to NLP — [AIMagazine]" /><published>2024-07-02T00:00:00+02:00</published><updated>2024-09-20T13:36:27+02:00</updated><id>http://localhost:4000/blogs/Introduction-to-NLP-AIMagazine</id><content type="html" xml:base="http://localhost:4000/blogs/2024-07-02-Introduction-to-NLP-AIMagazine/"><![CDATA[<p><strong>Reading_time:</strong> 5 min<br />
  <strong>Tags:</strong> [AI ,MachineLearning, NLP, ComputerVision, MLOps DeepLearning, DataScience, TechInnovation,FutureTech
]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#1-introduction-to-nlp--aimagazine" id="markdown-toc-1-introduction-to-nlp--aimagazine">1-Introduction to NLP — AIMagazine</a>    <ul>
      <li><a href="#dive-into-the-world-of-nlp" id="markdown-toc-dive-into-the-world-of-nlp">Dive into the World of NLP</a></li>
      <li><a href="#why-you-should-read-this" id="markdown-toc-why-you-should-read-this">Why You Should Read This</a></li>
      <li><a href="#stay-updated" id="markdown-toc-stay-updated">Stay Updated</a></li>
    </ul>
  </li>
  <li><a href="#read-more-on-aimagazine" id="markdown-toc-read-more-on-aimagazine"><strong>Read More on AIMagazine</strong></a></li>
</ul>

<h2 id="1-introduction-to-nlp--aimagazine">1-Introduction to NLP — AIMagazine</h2>

<h3 id="dive-into-the-world-of-nlp">Dive into the World of NLP</h3>

<p>Artificial intelligence, machine learning, natural language processing (NLP), and MLOps are evolving at an incredible pace. Staying informed about the latest developments, research, and trends is crucial for anyone in the tech field. To help you keep up, I’ve published a detailed article on NLP on my Substack <strong><a href="https://aboniasojasingarayar.substack.com">AIMagazine</a></strong>.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*NQzXDZ5yb0S4DIRJcpR-VQ.jpeg" alt="" /></p>

<p>Here’s a glimpse of what you can expect in this first chapter format article and what You’ll Learn:</p>

<p><strong>Introduction to NLP</strong></p>

<ul>
  <li>Understanding the basics and scope of NLP.</li>
</ul>

<p><strong>Significance of NLP</strong></p>

<ul>
  <li>Why NLP is essential in today’s tech landscape.</li>
</ul>

<p><strong>Applications and Historical Context</strong></p>

<ul>
  <li>Real-world uses of NLP and its evolution over time.</li>
</ul>

<p><strong>Mechanics of NLP</strong></p>

<ul>
  <li>Key techniques like tokenization, stemming, and tagging.</li>
</ul>

<p><strong>Challenges and Limitations</strong></p>

<ul>
  <li>The technical hurdles and complexities involved in NLP development.</li>
</ul>

<p><strong>Further Resources</strong></p>

<p>-Popular NLP datasets, libraries, and online courses to enhance your learning.</p>

<h3 id="why-you-should-read-this">Why You Should Read This</h3>

<p>This article aims to provide a foundational understanding of NLP and its applications. By the end of the article, you’ll be able to identify the different components of an NLP system and grasp the challenges involved in developing these systems. Whether you’re a beginner or someone looking to refresh your knowledge, this article is designed to offer valuable insights.</p>

<h3 id="stay-updated">Stay Updated</h3>

<p>I’m planning to write articles in a chapter format for an in-depth exploration of each topic, covering aspects like NLP, LLMs, fine-tuning, evaluation, RAG, information retrieval, deployment, knowledge graphs, and more. There are many other concepts to be added, and I will update them in subsequent posts.</p>

<h2 id="read-more-on-aimagazine"><strong>Read More on AIMagazine</strong></h2>

<p>To dive deeper into these topics, I invite you to read the full article on my substack <strong><a href="https://aboniasojasingarayar.substack.com">AIMagazine</a></strong>. Your feedback and engagement are highly valued, and subscribing will ensure you get updates directly in your inbox whenever new articles are published.</p>

<p>👉 <strong>Read the full <a href="https://aboniasojasingarayar.substack.com/p/8e1b5cd1-21ac-41df-884b-0df2cb26f905">article on Substack AIMagazine </a></strong></p>

<hr />
<p><strong><em>Thanks for Reading!</em></strong></p>

<p><strong><a href="https://abonia1.github.io/">Website/Newletter</a></strong></p>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Dive into the World of NLP.This article introduces the basics and scope of NLP, explaining what NLP is and why it matters in today's tech landscape. It explores real-world applications and provides historical context, showcasing the evolution and milestones of NLP. You'll also learn about the inner workings of NLP, including techniques like tokenization, stemming, and tagging. The article addresses the challenges and limitations in developing NLP systems, highlighting the technical hurdles and complexities. Additionally, it offers further resources, such as popular datasets, libraries, and online courses, to deepen your understanding.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/AIMagazine.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/AIMagazine.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">YOLO Segmentation Predictions to Labelme and Anylabeling-Compatible JSON</title><link href="http://localhost:4000/blogs/2024-05-08-YOLO-to-Labelme-and-Anylabeling-Compatible-JSON/" rel="alternate" type="text/html" title="YOLO Segmentation Predictions to Labelme and Anylabeling-Compatible JSON" /><published>2024-05-08T00:00:00+02:00</published><updated>2024-05-08T15:24:55+02:00</updated><id>http://localhost:4000/blogs/YOLO-to-Labelme-and-Anylabeling-Compatible-JSON</id><content type="html" xml:base="http://localhost:4000/blogs/2024-05-08-YOLO-to-Labelme-and-Anylabeling-Compatible-JSON/"><![CDATA[<p><strong>Reading_time:</strong> 5 min<br />
  <strong>Tags:</strong> [YOLO, ,Annotation, LabelMe, AnyLbaeling, ComputerVision, Json]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#installing-yolo-segment-to-labelme-converter" id="markdown-toc-installing-yolo-segment-to-labelme-converter">Installing YOLO Segment to LabelMe Converter</a></li>
  <li><a href="#installing-yolo-segment-to-labelme-converter-1" id="markdown-toc-installing-yolo-segment-to-labelme-converter-1">Installing YOLO Segment to LabelMe Converter</a>    <ul>
      <li><a href="#prerequisites" id="markdown-toc-prerequisites">Prerequisites</a></li>
      <li><a href="#installation" id="markdown-toc-installation">Installation</a>        <ul>
          <li><a href="#specifying-a-version" id="markdown-toc-specifying-a-version">Specifying a Version</a></li>
        </ul>
      </li>
      <li><a href="#using-a-virtual-environment" id="markdown-toc-using-a-virtual-environment">Using a Virtual Environment</a>        <ul>
          <li><a href="#creating-a-virtual-environment-with-venv" id="markdown-toc-creating-a-virtual-environment-with-venv">Creating a Virtual Environment with venv</a></li>
        </ul>
      </li>
      <li><a href="#sample-usage" id="markdown-toc-sample-usage">Sample Usage</a>        <ul>
          <li><a href="#customizing-the-model-and-confidence-score" id="markdown-toc-customizing-the-model-and-confidence-score">Customizing the Model and Confidence Score</a></li>
          <li><a href="#run" id="markdown-toc-run">Run</a></li>
        </ul>
      </li>
      <li><a href="#sample-output-in-anylabeling-annotation-tool" id="markdown-toc-sample-output-in-anylabeling-annotation-tool">Sample Output in Anylabeling Annotation Tool</a></li>
    </ul>
  </li>
  <li><a href="#thanks-for-reading" id="markdown-toc-thanks-for-reading"><em>Thanks for Reading!</em></a></li>
</ul>
<p><strong>Table of Contents:</strong></p>
<ul>
  <li><a href="#using-a-virtual-environment">Setting your environment)</a></li>
  <li><a href="#sample-usage">Sample Usage</a></li>
  <li><a href="#sample-output-in-anylabeling-annotation-tool">Sample Output in Annotation tool</a></li>
</ul>

<h1 id="installing-yolo-segment-to-labelme-converter">Installing YOLO Segment to LabelMe Converter</h1>

<p>I’m excited to announce the release of a new Python package, <strong><a href="https://github.com/Abonia1/yolosegment2labelme">yolosegment2labelme</a></strong>, which is now available on PyPI. This tool is specifically designed to streamline the conversion process of YOLO segmentation masks into a format that’s fully compatible with LabelMe and anylabeling, a widely recognized annotation tool for image segmentation tasks. In this guide, I’ll walk you through the installation process and provide you with a clear understanding of how to utilize this package effectively in your projects.</p>

<p><strong><img src="/assets/img/blog/yolosegment2labelme.png" alt="Source-Image by Author: Github - yolosegment2labelme " /></strong></p>

<h1 id="installing-yolo-segment-to-labelme-converter-1">Installing <a href="https://pypi.org/project/yolosegment2labelme/">YOLO Segment to LabelMe Converter</a></h1>

<p>This guide will walk you through the process of installing the <strong><a href="https://pypi.org/project/yolosegment2labelme/">yolosegment2labelme</a></strong> package, a tool designed to convert YOLO segmentation masks into a format compatible with LabelMe, a popular annotation tool for image segmentation tasks.</p>

<p>The source code for <code class="language-plaintext highlighter-rouge">yolosegment2labelme</code> can be found on <strong><a href="https://github.com/Abonia1/yolosegment2labelme">GitHub</a></strong>, where you can contribute, report issues, or explore the codebase.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before you begin, ensure you have the following prerequisites installed on your system:</p>

<ul>
  <li>Python 3.x</li>
  <li>pip (Python package installer)</li>
</ul>

<h2 id="installation">Installation</h2>

<p>To install the <code class="language-plaintext highlighter-rouge">yolosegment2labelme</code> package, you can use the <code class="language-plaintext highlighter-rouge">pip</code> command. Open your terminal or command prompt and execute the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> pip <span class="nb">install </span>yolosegment2labelme
</code></pre></div></div>

<p>If you’re using Python 3 and have both Python 2 and Python 3 installed, you might need to use <code class="language-plaintext highlighter-rouge">python3</code> instead:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> pip <span class="nb">install </span>yolosegment2labelme
</code></pre></div></div>

<h3 id="specifying-a-version">Specifying a Version</h3>

<p>If you need a specific version of the package, you can specify it by appending <code class="language-plaintext highlighter-rouge">==&lt;version&gt;</code> to the package name. For example, to install version 0.0.4, you would use:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> pip <span class="nb">install </span><span class="nv">yolosegment2labelme</span><span class="o">==</span>0.0.4
</code></pre></div></div>

<h2 id="using-a-virtual-environment">Using a Virtual Environment</h2>

<p>It’s recommended to use a virtual environment when installing Python packages to avoid conflicts between package versions. You can create a virtual environment using <code class="language-plaintext highlighter-rouge">venv</code> (included in Python 3.3 and later) or <code class="language-plaintext highlighter-rouge">virtualenv</code>.</p>

<h3 id="creating-a-virtual-environment-with-venv">Creating a Virtual Environment with venv</h3>

<ol>
  <li>Create a virtual environment:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> venv myenv
</code></pre></div></div>

<ol>
  <li>Activate the virtual environment:</li>
</ol>

<ul>
  <li>On Windows:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>myenv<span class="se">\S</span>cripts<span class="se">\a</span>ctivate
</code></pre></div></div>

<ul>
  <li>On macOS and Linux:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>myenv/bin/activate
</code></pre></div></div>

<ol>
  <li>With the virtual environment activated, install the <code class="language-plaintext highlighter-rouge">yolosegment2labelme</code> package:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> pip <span class="nb">install </span>yolosegment2labelme
</code></pre></div></div>
<h2 id="sample-usage">Sample Usage</h2>

<p>To convert segmentation masks from a directory of images using the default model (<code class="language-plaintext highlighter-rouge">yolo8n-seg</code>) and a default confidence score of 0.2, you can use the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python yolosegment2labelme.py <span class="nt">--images</span> ../images
</code></pre></div></div>

<p>In this example, replace <code class="language-plaintext highlighter-rouge">../images</code> with the path to the directory containing your YOLO segmentation mask images. The script will process all images in the specified directory, applying the default model and confidence score to generate LabelMe-compatible annotations.</p>

<h3 id="customizing-the-model-and-confidence-score">Customizing the Model and Confidence Score</h3>

<p>If you prefer to use a specific YOLO model or your custom fine-tuned model and adjust the confidence score for the predictions, you can do so by specifying the <code class="language-plaintext highlighter-rouge">--model</code> and <code class="language-plaintext highlighter-rouge">--conf</code> options:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python yolosegment2labelme.py <span class="nt">--model</span> yolov5n-seg.pt <span class="nt">--images</span> ../images <span class="nt">--conf</span> 0.3
</code></pre></div></div>

<p>In this command:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">--model yolov8n-seg.pt</code> specifies the path to the YOLO model file you wish to use. Replace <code class="language-plaintext highlighter-rouge">yolov8n-seg.pt</code> with the path to your desired model file.</li>
  <li><code class="language-plaintext highlighter-rouge">--images ../images</code> indicates the directory containing the images you want to process. Replace <code class="language-plaintext highlighter-rouge">../images</code> with the actual path to your image directory.</li>
  <li><code class="language-plaintext highlighter-rouge">--conf 0.3</code> sets the confidence score threshold for the predictions. Adjust this value according to your needs; higher values require more confident predictions to be considered.</li>
</ul>

<h3 id="run">Run</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">yolosegment2labelme</span> <span class="o">--</span><span class="n">model</span> <span class="n">yolov8n</span><span class="o">-</span><span class="n">seg</span><span class="p">.</span><span class="n">pt</span> <span class="o">--</span><span class="n">images</span> <span class="p">..</span><span class="o">/</span><span class="n">images</span>  <span class="o">--</span><span class="n">conf</span> <span class="mf">0.3</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image 1/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/1.jpg: 448x640 1 bear, 118.4ms
image 2/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg: 448x640 1 cup, 1 laptop, 1 cell phone, 2 books, 93.8ms
image 3/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/2.jpg: 640x448 1 dog, 83.3ms
image 4/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg: 480x640 4 chairs, 97.9ms
image 5/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg: 544x640 2 persons, 2 sports balls, 128.5ms
image 6/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg: 448x640 4 persons, 1 sports ball, 3 kites, 81.2ms
image 7/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/6.jpg: 448x640 1 bird, 85.1ms
image 8/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg: 416x640 1 person, 1 dog, 84.8ms
image 9/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg: 448x640 11 persons, 5 cars, 4 traffic lights, 88.8ms
image 10/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg: 448x640 1 person, 79.8ms
Speed: 1.5ms preprocess, 94.1ms inference, 78.1ms postprocess per image at shape (1, 3, 448, 640)
Results saved to [1mruns/segment/predict7[0m
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/1.jpg with label bear
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label cup
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label laptop
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label cell phone
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/2.jpg with label dog
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/6.jpg with label bird
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg with label dog
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg with label person
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">python</span> <span class="n">yolosegment2labelme</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">images</span> <span class="p">..</span><span class="o">/</span><span class="n">images</span> 
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image 1/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/1.jpg: 448x640 1 bear, 111.5ms
image 2/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg: 448x640 1 cup, 1 laptop, 1 cell phone, 5 books, 149.6ms
image 3/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/2.jpg: 640x448 1 dog, 92.1ms
image 4/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg: 480x640 4 chairs, 101.8ms
image 5/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg: 544x640 2 persons, 2 sports balls, 119.7ms
image 6/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg: 448x640 6 persons, 1 sports ball, 3 kites, 91.8ms
image 7/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/6.jpg: 448x640 1 bird, 83.4ms
image 8/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg: 416x640 1 person, 1 dog, 78.1ms
image 9/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg: 448x640 19 persons, 7 cars, 4 traffic lights, 85.0ms
image 10/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg: 448x640 1 person, 1 tv, 88.7ms
Speed: 1.1ms preprocess, 100.2ms inference, 72.5ms postprocess per image at shape (1, 3, 448, 640)
Results saved to [1mruns/segment/predict4[0m
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/1.jpg with label bear
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label cup
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label laptop
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label cell phone
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/2.jpg with label dog
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/6.jpg with label bird
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg with label dog
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg with label tv
</code></pre></div></div>

<div align="center">
    📦 <a href="https://github.com/Abonia1/yolosegment2labelme/blob/main/yolosegment2labelme/example.ipynb" target="_blank"><b>Sample Notebook</b></a>
</div>

<h2 id="sample-output-in-anylabeling-annotation-tool">Sample Output in Anylabeling Annotation Tool</h2>

<p>Below are examples of image annotations created in json using yolosegment2labelme and viewed in the <a href="https://github.com/vietanhdev/anylabeling">Anylabeling</a> annotation tool:</p>

<table>
  <thead>
    <tr>
      <th>Sample Image 1</th>
      <th>Sample Image 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/sample1.png" alt="Sample Image 1" /></td>
      <td><img src="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/sample2.png" alt="Sample Image 2" /></td>
    </tr>
    <tr>
      <td>Sample Annotation for Image 1</td>
      <td>Sample Annotation for Image 2</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>Sample Image 3</th>
      <th>Sample Image 4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/sample3.png" alt="Sample Image 3" /></td>
      <td><img src="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/sample4.png" alt="Sample Image 4" /></td>
    </tr>
    <tr>
      <td>Sample Annotation for Image 3</td>
      <td>Sample Annotation for Image 4</td>
    </tr>
  </tbody>
</table>

<p>By following these steps, you should now have the <code class="language-plaintext highlighter-rouge">yolosegment2labelme</code> package installed on your system, ready to convert YOLO segmentation masks into LabelMe-compatible formats.Hope this tool will be a helpful asset for researchers and developers working with image segmentation tasks, streamlining the process of preparing data for further analysis or model training.Stay tuned for our upcoming article! Until then, catch you in another exciting article!</p>

<hr />
<h1 id="thanks-for-reading"><em>Thanks for Reading!</em></h1>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[yolosegment2labelme - a Python package that allows you to convert YOLO segmentation prediction results to LabelMe and anylabeling JSON format. This tool facilitates the annotation easy.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/logo.png" /><media:content medium="image" url="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>