<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-07-01T16:35:44+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Abonia Sojasingarayar</title><subtitle>Holds a Masters degree in Artificial Intelligence and boasts over 7 years of experience. With a robust background in crafting and implementing ML systems across diverse domains, including cybersecurity, banking, transit,cosmetics &amp; hygiene research, and e-commerce.
</subtitle><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><entry><title type="html">YOLO Segmentation Predictions to Labelme and Anylabeling-Compatible JSON</title><link href="http://localhost:4000/blogs/2024-05-08-YOLO-to-Labelme-and-Anylabeling-Compatible-JSON/" rel="alternate" type="text/html" title="YOLO Segmentation Predictions to Labelme and Anylabeling-Compatible JSON" /><published>2024-05-08T00:00:00+02:00</published><updated>2024-05-08T15:24:55+02:00</updated><id>http://localhost:4000/blogs/YOLO-to-Labelme-and-Anylabeling-Compatible-JSON</id><content type="html" xml:base="http://localhost:4000/blogs/2024-05-08-YOLO-to-Labelme-and-Anylabeling-Compatible-JSON/"><![CDATA[<p><strong>Reading_time:</strong> 5 min<br />
  <strong>Tags:</strong> [YOLO, ,Annotation, LabelMe, AnyLbaeling, ComputerVision, Json]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#installing-yolo-segment-to-labelme-converter" id="markdown-toc-installing-yolo-segment-to-labelme-converter">Installing YOLO Segment to LabelMe Converter</a></li>
  <li><a href="#installing-yolo-segment-to-labelme-converter-1" id="markdown-toc-installing-yolo-segment-to-labelme-converter-1">Installing YOLO Segment to LabelMe Converter</a>    <ul>
      <li><a href="#prerequisites" id="markdown-toc-prerequisites">Prerequisites</a></li>
      <li><a href="#installation" id="markdown-toc-installation">Installation</a>        <ul>
          <li><a href="#specifying-a-version" id="markdown-toc-specifying-a-version">Specifying a Version</a></li>
        </ul>
      </li>
      <li><a href="#using-a-virtual-environment" id="markdown-toc-using-a-virtual-environment">Using a Virtual Environment</a>        <ul>
          <li><a href="#creating-a-virtual-environment-with-venv" id="markdown-toc-creating-a-virtual-environment-with-venv">Creating a Virtual Environment with venv</a></li>
        </ul>
      </li>
      <li><a href="#sample-usage" id="markdown-toc-sample-usage">Sample Usage</a>        <ul>
          <li><a href="#customizing-the-model-and-confidence-score" id="markdown-toc-customizing-the-model-and-confidence-score">Customizing the Model and Confidence Score</a></li>
          <li><a href="#run" id="markdown-toc-run">Run</a></li>
        </ul>
      </li>
      <li><a href="#sample-output-in-anylabeling-annotation-tool" id="markdown-toc-sample-output-in-anylabeling-annotation-tool">Sample Output in Anylabeling Annotation Tool</a></li>
    </ul>
  </li>
  <li><a href="#thanks-for-reading" id="markdown-toc-thanks-for-reading"><em>Thanks for Reading!</em></a></li>
</ul>
<p><strong>Table of Contents:</strong></p>
<ul>
  <li><a href="#using-a-virtual-environment">Setting your environment)</a></li>
  <li><a href="#sample-usage">Sample Usage</a></li>
  <li><a href="#sample-output-in-anylabeling-annotation-tool">Sample Output in Annotation tool</a></li>
</ul>

<h1 id="installing-yolo-segment-to-labelme-converter">Installing YOLO Segment to LabelMe Converter</h1>

<p>I‚Äôm excited to announce the release of a new Python package, <strong><a href="https://github.com/Abonia1/yolosegment2labelme">yolosegment2labelme</a></strong>, which is now available on PyPI. This tool is specifically designed to streamline the conversion process of YOLO segmentation masks into a format that‚Äôs fully compatible with LabelMe and anylabeling, a widely recognized annotation tool for image segmentation tasks. In this guide, I‚Äôll walk you through the installation process and provide you with a clear understanding of how to utilize this package effectively in your projects.</p>

<p><strong><img src="/assets/img/blog/yolosegment2labelme.png" alt="Source-Image by Author: Github - yolosegment2labelme " /></strong></p>

<h1 id="installing-yolo-segment-to-labelme-converter-1">Installing <a href="https://pypi.org/project/yolosegment2labelme/">YOLO Segment to LabelMe Converter</a></h1>

<p>This guide will walk you through the process of installing the <strong><a href="https://pypi.org/project/yolosegment2labelme/">yolosegment2labelme</a></strong> package, a tool designed to convert YOLO segmentation masks into a format compatible with LabelMe, a popular annotation tool for image segmentation tasks.</p>

<p>The source code for <code class="language-plaintext highlighter-rouge">yolosegment2labelme</code> can be found on <strong><a href="https://github.com/Abonia1/yolosegment2labelme">GitHub</a></strong>, where you can contribute, report issues, or explore the codebase.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before you begin, ensure you have the following prerequisites installed on your system:</p>

<ul>
  <li>Python 3.x</li>
  <li>pip (Python package installer)</li>
</ul>

<h2 id="installation">Installation</h2>

<p>To install the <code class="language-plaintext highlighter-rouge">yolosegment2labelme</code> package, you can use the <code class="language-plaintext highlighter-rouge">pip</code> command. Open your terminal or command prompt and execute the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> pip <span class="nb">install </span>yolosegment2labelme
</code></pre></div></div>

<p>If you‚Äôre using Python 3 and have both Python 2 and Python 3 installed, you might need to use <code class="language-plaintext highlighter-rouge">python3</code> instead:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> pip <span class="nb">install </span>yolosegment2labelme
</code></pre></div></div>

<h3 id="specifying-a-version">Specifying a Version</h3>

<p>If you need a specific version of the package, you can specify it by appending <code class="language-plaintext highlighter-rouge">==&lt;version&gt;</code> to the package name. For example, to install version 0.0.4, you would use:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> pip <span class="nb">install </span><span class="nv">yolosegment2labelme</span><span class="o">==</span>0.0.4
</code></pre></div></div>

<h2 id="using-a-virtual-environment">Using a Virtual Environment</h2>

<p>It‚Äôs recommended to use a virtual environment when installing Python packages to avoid conflicts between package versions. You can create a virtual environment using <code class="language-plaintext highlighter-rouge">venv</code> (included in Python 3.3 and later) or <code class="language-plaintext highlighter-rouge">virtualenv</code>.</p>

<h3 id="creating-a-virtual-environment-with-venv">Creating a Virtual Environment with venv</h3>

<ol>
  <li>Create a virtual environment:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> venv myenv
</code></pre></div></div>

<ol>
  <li>Activate the virtual environment:</li>
</ol>

<ul>
  <li>On Windows:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>myenv<span class="se">\S</span>cripts<span class="se">\a</span>ctivate
</code></pre></div></div>

<ul>
  <li>On macOS and Linux:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>myenv/bin/activate
</code></pre></div></div>

<ol>
  <li>With the virtual environment activated, install the <code class="language-plaintext highlighter-rouge">yolosegment2labelme</code> package:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> pip <span class="nb">install </span>yolosegment2labelme
</code></pre></div></div>
<h2 id="sample-usage">Sample Usage</h2>

<p>To convert segmentation masks from a directory of images using the default model (<code class="language-plaintext highlighter-rouge">yolo8n-seg</code>) and a default confidence score of 0.2, you can use the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python yolosegment2labelme.py <span class="nt">--images</span> ../images
</code></pre></div></div>

<p>In this example, replace <code class="language-plaintext highlighter-rouge">../images</code> with the path to the directory containing your YOLO segmentation mask images. The script will process all images in the specified directory, applying the default model and confidence score to generate LabelMe-compatible annotations.</p>

<h3 id="customizing-the-model-and-confidence-score">Customizing the Model and Confidence Score</h3>

<p>If you prefer to use a specific YOLO model or your custom fine-tuned model and adjust the confidence score for the predictions, you can do so by specifying the <code class="language-plaintext highlighter-rouge">--model</code> and <code class="language-plaintext highlighter-rouge">--conf</code> options:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python yolosegment2labelme.py <span class="nt">--model</span> yolov5n-seg.pt <span class="nt">--images</span> ../images <span class="nt">--conf</span> 0.3
</code></pre></div></div>

<p>In this command:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">--model yolov8n-seg.pt</code> specifies the path to the YOLO model file you wish to use. Replace <code class="language-plaintext highlighter-rouge">yolov8n-seg.pt</code> with the path to your desired model file.</li>
  <li><code class="language-plaintext highlighter-rouge">--images ../images</code> indicates the directory containing the images you want to process. Replace <code class="language-plaintext highlighter-rouge">../images</code> with the actual path to your image directory.</li>
  <li><code class="language-plaintext highlighter-rouge">--conf 0.3</code> sets the confidence score threshold for the predictions. Adjust this value according to your needs; higher values require more confident predictions to be considered.</li>
</ul>

<h3 id="run">Run</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">yolosegment2labelme</span> <span class="o">--</span><span class="n">model</span> <span class="n">yolov8n</span><span class="o">-</span><span class="n">seg</span><span class="p">.</span><span class="n">pt</span> <span class="o">--</span><span class="n">images</span> <span class="p">..</span><span class="o">/</span><span class="n">images</span>  <span class="o">--</span><span class="n">conf</span> <span class="mf">0.3</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image 1/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/1.jpg: 448x640 1 bear, 118.4ms
image 2/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg: 448x640 1 cup, 1 laptop, 1 cell phone, 2 books, 93.8ms
image 3/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/2.jpg: 640x448 1 dog, 83.3ms
image 4/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg: 480x640 4 chairs, 97.9ms
image 5/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg: 544x640 2 persons, 2 sports balls, 128.5ms
image 6/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg: 448x640 4 persons, 1 sports ball, 3 kites, 81.2ms
image 7/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/6.jpg: 448x640 1 bird, 85.1ms
image 8/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg: 416x640 1 person, 1 dog, 84.8ms
image 9/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg: 448x640 11 persons, 5 cars, 4 traffic lights, 88.8ms
image 10/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg: 448x640 1 person, 79.8ms
Speed: 1.5ms preprocess, 94.1ms inference, 78.1ms postprocess per image at shape (1, 3, 448, 640)
Results saved to [1mruns/segment/predict7[0m
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/1.jpg with label bear
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label cup
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label laptop
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label cell phone
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/2.jpg with label dog
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/6.jpg with label bird
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg with label dog
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg with label person
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">python</span> <span class="n">yolosegment2labelme</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">images</span> <span class="p">..</span><span class="o">/</span><span class="n">images</span> 
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image 1/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/1.jpg: 448x640 1 bear, 111.5ms
image 2/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg: 448x640 1 cup, 1 laptop, 1 cell phone, 5 books, 149.6ms
image 3/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/2.jpg: 640x448 1 dog, 92.1ms
image 4/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg: 480x640 4 chairs, 101.8ms
image 5/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg: 544x640 2 persons, 2 sports balls, 119.7ms
image 6/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg: 448x640 6 persons, 1 sports ball, 3 kites, 91.8ms
image 7/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/6.jpg: 448x640 1 bird, 83.4ms
image 8/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg: 416x640 1 person, 1 dog, 78.1ms
image 9/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg: 448x640 19 persons, 7 cars, 4 traffic lights, 85.0ms
image 10/10 /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg: 448x640 1 person, 1 tv, 88.7ms
Speed: 1.1ms preprocess, 100.2ms inference, 72.5ms postprocess per image at shape (1, 3, 448, 640)
Results saved to [1mruns/segment/predict4[0m
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/1.jpg with label bear
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label cup
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label laptop
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label cell phone
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/10.jpg with label book
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/2.jpg with label dog
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/3.jpg with label chair
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/4.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label sports ball
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label kite
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/5.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/6.jpg with label bird
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg with label dog
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/7.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label traffic light
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label car
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/8.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg with label person
Polygon information saved for /Users/aboniasojasingarayar/Documents/Github/yolosegment2labelme/yolosegment2labelme/../images/9.jpg with label tv
</code></pre></div></div>

<div align="center">
    üì¶ <a href="https://github.com/Abonia1/yolosegment2labelme/blob/main/yolosegment2labelme/example.ipynb" target="_blank"><b>Sample Notebook</b></a>
</div>

<h2 id="sample-output-in-anylabeling-annotation-tool">Sample Output in Anylabeling Annotation Tool</h2>

<p>Below are examples of image annotations created in json using yolosegment2labelme and viewed in the <a href="https://github.com/vietanhdev/anylabeling">Anylabeling</a> annotation tool:</p>

<table>
  <thead>
    <tr>
      <th>Sample Image 1</th>
      <th>Sample Image 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/sample1.png" alt="Sample Image 1" /></td>
      <td><img src="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/sample2.png" alt="Sample Image 2" /></td>
    </tr>
    <tr>
      <td>Sample Annotation for Image 1</td>
      <td>Sample Annotation for Image 2</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>Sample Image 3</th>
      <th>Sample Image 4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/sample3.png" alt="Sample Image 3" /></td>
      <td><img src="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/sample4.png" alt="Sample Image 4" /></td>
    </tr>
    <tr>
      <td>Sample Annotation for Image 3</td>
      <td>Sample Annotation for Image 4</td>
    </tr>
  </tbody>
</table>

<p>By following these steps, you should now have the <code class="language-plaintext highlighter-rouge">yolosegment2labelme</code> package installed on your system, ready to convert YOLO segmentation masks into LabelMe-compatible formats.Hope this tool will be a helpful asset for researchers and developers working with image segmentation tasks, streamlining the process of preparing data for further analysis or model training.Stay tuned for our upcoming article! Until then, catch you in another exciting article!</p>

<hr />
<h1 id="thanks-for-reading"><em>Thanks for Reading!</em></h1>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[yolosegment2labelme - a Python package that allows you to convert YOLO segmentation prediction results to LabelMe and anylabeling JSON format. This tool facilitates the annotation easy.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/logo.png" /><media:content medium="image" url="https://raw.githubusercontent.com/Abonia1/yolosegment2labelme/main/images/labelme_test/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Summarization-with-LangChain:Stuff ‚Äî Map_reduce ‚Äî Refine</title><link href="http://localhost:4000/blogs/2024-04-22-Summarization-with-LangChain/" rel="alternate" type="text/html" title="Summarization-with-LangChain:Stuff ‚Äî Map_reduce ‚Äî Refine" /><published>2024-04-22T00:00:00+02:00</published><updated>2024-05-08T15:24:55+02:00</updated><id>http://localhost:4000/blogs/Summarization-with-LangChain</id><content type="html" xml:base="http://localhost:4000/blogs/2024-04-22-Summarization-with-LangChain/"><![CDATA[<p><strong><a href="https://youtu.be/w6wOhSThnoo">Link to the complete hands-on tutorial on Summarizer techniques</a></strong></p>

<p><strong>Reading_time:</strong> 7 min<br />
  <strong>Tags:</strong> [Langchain, LLM, GenAI, Summarizer]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#summarization-techniques" id="markdown-toc-summarization-techniques">Summarization Techniques</a></li>
  <li><a href="#stuff-chain" id="markdown-toc-stuff-chain">Stuff Chain</a></li>
  <li><a href="#map-reduce-method" id="markdown-toc-map-reduce-method">Map-Reduce Method</a></li>
  <li><a href="#refine-method" id="markdown-toc-refine-method">Refine Method</a></li>
  <li><a href="#choosing-the-right-technique" id="markdown-toc-choosing-the-right-technique">Choosing the Right Technique</a></li>
  <li><a href="#thanks-for-reading" id="markdown-toc-thanks-for-reading"><em>Thanks for Reading!</em></a></li>
</ul>
<p><strong>Table of Contents:</strong></p>
<ul>
  <li><a href="#stuff-chain">Stuff Chain Method</a></li>
  <li><a href="#map-reduce-method">Map-Reduce Method)</a></li>
  <li><a href="#refine-method">Refine Method</a></li>
</ul>

<p>I recently wrapped a tutorial on summarization techniques in LangChain. This article covers the basic usage of document summarization techniques and provides insights into various summarization methods. Additionally, to learn more and to explore how to validate intermediate results from the output of each of these techniques. For those interested in delving deeper into this topic, I invite you to explore the complete and comprehensive tutorial available <strong><a href="https://youtu.be/w6wOhSThnoo">here</a></strong>.</p>

<p>Summarization is a critical aspect of natural language processing (NLP), enabling the condensation of large volumes of text into concise summaries. LangChain, a powerful tool in the NLP domain, offers three distinct summarization techniques: stuff, map_reduce, and refine. Each method has its unique advantages and limitations, making them suitable for different scenarios. This article delves into the details of these techniques, their pros and cons, and the ideal scenarios for their application.</p>

<p>Complete implementation code and data used in the tutorial available in the below <strong><a href="https://github.com/Abonia1/Langchain-Summarizer">repository</a></strong>.</p>

<h2 id="summarization-techniques">Summarization Techniques</h2>

<p><img src="https://cdn-images-1.medium.com/max/3310/0*F6xtU1qhHfVSqQdO.png" alt="Courtesy of langchain" />
<strong><a href="https://python.langchain.com/docs/use_cases/summarization">Summarization | ü¶úÔ∏èüîó Langchain</a></strong></p>

<h2 id="stuff-chain">Stuff Chain</h2>

<p>The stuff chain is particularly effective for handling large documents. It works by converting the document into smaller chunks, processing each chunk individually, and then combining the summaries to generate a final summary. This method is ideal for managing huge files and can be facilitated with the help of a recursive character text splitter.</p>

<p><strong>Pros:</strong></p>

<ul>
  <li>
    <p>Efficiently handles large documents.</p>
  </li>
  <li>
    <p>Allows for chunk-wise summarization, making it suitable for managing huge files.</p>
  </li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
  <li>LLM context window</li>
</ul>

<p>Here‚Äôs an example of how to implement the stuff chain method:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains.llm import LLMChain
from langchain.prompts import PromptTemplate

#Define prompt
prompt_template = """Write a concise summary of the following:
"{text}"
CONCISE SUMMARY:"""
prompt = PromptTemplate.from_template(prompt_template)
#Define LLM chain
llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo-16k")
llm_chain = LLMChain(llm=llm, prompt=prompt)
#Define StuffDocumentsChain
stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name="text")
docs = loader.load()
print(stuff_chain.run(docs))
</code></pre></div></div>

<h2 id="map-reduce-method">Map-Reduce Method</h2>

<p>The Map-Reduce method involves summarizing each document individually (map step) and then combining these summaries into a final summary (reduce step). This approach is more scalable and can handle larger volumes of text.The map_reduce technique is designed for summarizing large documents that exceed the token limit of the language model. It involves dividing the document into chunks, generating summaries for each chunk, and then combining these summaries to create a final summary. This method is efficient for handling large files and significantly reduces processing time.</p>

<p><strong>Pros:</strong></p>

<ul>
  <li>
    <p>Effectively handles large documents by dividing them into manageable chunks.</p>
  </li>
  <li>
    <p>Reduces processing time by processing chunks individually.</p>
  </li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
  <li>Requires extra steps in combining individual summaries, which can add complexity to the process.</li>
</ul>

<p>Here‚Äôs an example of how to implement the Map-Reduce method:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain
from langchain_text_splitters import CharacterTextSplitter

# Map
map_template = """The following is a set of documents
{docs}
Based on this list of docs, please identify the main themes 
Helpful Answer:"""
map_prompt = PromptTemplate.from_template(map_template)
map_chain = LLMChain(llm=llm, prompt=map_prompt)
# Reduce
reduce_template = """The following is set of summaries:
{docs}
Take these and distill it into a final, consolidated summary of the main themes. 
Helpful Answer:"""
reduce_prompt = PromptTemplate.from_template(reduce_template)
reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)
# Combine documents by mapping a chain over them, then combining results
map_reduce_chain = MapReduceDocumentsChain(
    llm_chain=map_chain,
    reduce_documents_chain=reduce_documents_chain,
    document_variable_name="docs",
    return_intermediate_steps=False,
)
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=0)
split_docs = text_splitter.split_documents(docs)
print(map_reduce_chain.run(split_docs))
</code></pre></div></div>

<h2 id="refine-method">Refine Method</h2>

<p>The Refine method iteratively updates its answer by looping over the input documents. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer. This method is useful for refining a summary based on new context.The refine technique is a simpler alternative to the map_reduce technique. It involves generating a summary for the first chunk, combining it with the second chunk, generating another summary, and continuing this process until a final summary is achieved. This method is suitable for large documents but requires less complexity compared to map_reduce.</p>

<p><strong>Pros:</strong></p>

<ul>
  <li>
    <p>Simpler than the map_reduce technique.</p>
  </li>
  <li>
    <p>Achieves similar results for large documents with less complexity.</p>
  </li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
  <li>Limited functionality compared to other techniques.</li>
</ul>

<p>Here‚Äôs how you can implement the Refine method:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from langchain.chains.summarize import load_summarize_chain
prompt = """
                  Please provide a summary of the following text.
                  TEXT: {text}
                  SUMMARY:
                  """

question_prompt = PromptTemplate(
    template=question_prompt_template, input_variables=["text"]
)

refine_prompt_template = """
              Write a concise summary of the following text delimited by triple backquotes.
              Return your response in bullet points which covers the key points of the text.
              ```{text}```
              BULLET POINT SUMMARY:
              """

refine_template = PromptTemplate(
    template=refine_prompt_template, input_variables=["text"]

# Load refine chain
chain = load_summarize_chain(
    llm=llm,
    chain_type="refine",
    question_prompt=question_prompt,
    refine_prompt=refine_prompt,
    return_intermediate_steps=True,
    input_key="input_documents",
    output_key="output_text",
)
result = chain({"input_documents": split_docs}, return_only_outputs=True)
</code></pre></div></div>

<h2 id="choosing-the-right-technique">Choosing the Right Technique</h2>

<p>The choice of summarization technique depends on the specific requirements of the task at hand. For large documents, the map_reduce and refine techniques are recommended due to their ability to handle chunk-wise summarization efficiently. The stuff chain is particularly useful for documents that are too large to be processed in a single go, offering a practical solution for managing huge files.</p>

<p>Each method has its advantages and is suitable for different scenarios. The Stuff method is straightforward but may not scale well with large volumes of text. The Map-Reduce method is more scalable and can handle larger documents but requires more setup. The Refine method is useful for iteratively refining a summary based on new context, making it a good choice for dynamic summarization tasks.</p>

<blockquote>
  <h1 id="thanks-for-reading"><em>Thanks for Reading!</em></h1>
</blockquote>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[I recently wrapped a tutorial on summarization techniques in LangChain. This article covers the basic usage of document summarization techniques and provides insights into various summarization methods. Additionally, to learn more and to explore how to validate intermediate results from the output of each of these techniques.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cdn-images-1.medium.com/max/3840/1*uDxlmvrPL0X5i4yd8iDGBw.png" /><media:content medium="image" url="https://cdn-images-1.medium.com/max/3840/1*uDxlmvrPL0X5i4yd8iDGBw.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How I Passed the GCP Professional Data Engineer Exam</title><link href="http://localhost:4000/blogs/2024-04-09-How-I-Passed-the-GCP-Professional/" rel="alternate" type="text/html" title="How I Passed the GCP Professional Data Engineer Exam" /><published>2024-04-09T00:00:00+02:00</published><updated>2024-04-12T10:28:22+02:00</updated><id>http://localhost:4000/blogs/How-I-Passed-the-GCP-Professional</id><content type="html" xml:base="http://localhost:4000/blogs/2024-04-09-How-I-Passed-the-GCP-Professional/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [GCP, Data Engineering, Cloud, Certification]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#1-basic-concept-from-udemy-courses" id="markdown-toc-1-basic-concept-from-udemy-courses">1. Basic concept from udemy courses</a></li>
  <li><a href="#2google-official-video-coursestheory--practical" id="markdown-toc-2google-official-video-coursestheory--practical">2.Google official video courses(Theory + Practical)</a></li>
  <li><a href="#3-books-and-resource" id="markdown-toc-3-books-and-resource">3. Books and resource</a></li>
  <li><a href="#4-official-documentation" id="markdown-toc-4-official-documentation">4. Official documentation</a></li>
  <li><a href="#5-quizzesexamtopicsmock-test" id="markdown-toc-5-quizzesexamtopicsmock-test">5. Quizzes/ExamTopics/Mock test</a></li>
  <li><a href="#6-cheatsheet" id="markdown-toc-6-cheatsheet">6. <strong><em>Cheatsheet</em></strong></a></li>
  <li><a href="#7-notes" id="markdown-toc-7-notes">7. Notes</a></li>
  <li><a href="#8-tips" id="markdown-toc-8-tips">8. Tips</a></li>
  <li><a href="#9-assessment" id="markdown-toc-9-assessment">9. Assessment</a></li>
  <li><a href="#colclusion" id="markdown-toc-colclusion">Colclusion:</a></li>
  <li><a href="#success-is-easy-to-achieve-once-you-set-your-mind-on-a-specific-goal" id="markdown-toc-success-is-easy-to-achieve-once-you-set-your-mind-on-a-specific-goal">Success is easy to achieve once you set your mind on a specific goal.</a></li>
  <li><a href="#--atticus-aristotle" id="markdown-toc---atticus-aristotle">- Atticus Aristotle</a></li>
</ul>
<p><strong>Table of Contents:</strong></p>
<ul>
  <li><a href="#1-basic-concept-from-udemy-courses">Basic concept from udemy courses</a></li>
  <li><a href="#2google-official-video-coursestheory--practical">Google official video courses(Theory + Practical)</a></li>
</ul>

<p><strong>In this article , I would like to share my experiences for preparing and taking the exam. I hope they‚Äôll help you in one way or another.</strong></p>

<p>Google Cloud Certified Professional Data Engineer exam tests your ability to design, deploy, monitor, and adapt services and infrastructure for data-driven decision-making.</p>

<p>The whole preparation took me nearly 2 months , but with some more focus time I could‚Äôve done it in 1 month. Following steps that I have taken to pass the exam :</p>

<ul>
  <li>
    <p>Basic concept from udemy course</p>
  </li>
  <li>
    <p>Google official video courses(Theory + Practical)</p>
  </li>
  <li>
    <p>Books and resources</p>
  </li>
  <li>
    <p>Official documentation</p>
  </li>
  <li>
    <p>Quizzes/ExamTopics/Mock test</p>
  </li>
  <li>
    <p>Cheatsheet</p>
  </li>
  <li>
    <p>Notes</p>
  </li>
  <li>
    <p>Assessment</p>
  </li>
  <li>
    <p>Tips</p>
  </li>
</ul>

<h3 id="1-basic-concept-from-udemy-courses">1. Basic concept from udemy courses</h3>

<p>I have followed the course <strong><em><a href="https://www.udemy.com/course/google-cloud-professional-data-engineer-get-certified/">Google Cloud Professional Data Engineer: Get Certified 2022</a></em></strong> by Dan Sullivan. I have followed all the classes in this course.As for public it costs 79,99‚Ç¨ and it covers following topics:</p>

<ul>
  <li>
    <p>Build scalable, reliable data pipelines</p>
  </li>
  <li>
    <p>Choose appropriate storage systems, including relational, NoSQL and analytical databases</p>
  </li>
  <li>
    <p>Apply multiple types of machine learning techniques to different use cases</p>
  </li>
  <li>
    <p>Deploy machine learning models in production</p>
  </li>
  <li>
    <p>Monitor data pipelines and machine learning models</p>
  </li>
  <li>
    <p>Design scalable, resilient distributed data intensive applications</p>
  </li>
  <li>
    <p>Migrate data warehouse from on-premises to Google Cloud</p>
  </li>
  <li>
    <p>Evaluate and improve the quality of machine learning models</p>
  </li>
  <li>
    <p>Grasp fundamental concepts in machine learning, such as backpropagation, feature engineering, overfitting and underfitting.</p>
  </li>
</ul>

<p>At the end of the course there will be 50 sample questions to solve which gives you 2 hours as like you are in exam.</p>

<h3 id="2google-official-video-coursestheory--practical">2.Google official video courses(Theory + Practical)</h3>

<p>I recommend to go through all the <strong><em><a href="http://Professional Data Engineer">Professional Data Engineer Google course</a></em></strong> videos and practicals from the official site.</p>

<p>Official guide for the data engineer exam is <a href="https://cloud.google.com/certification/guides/data-engineer">here</a>.</p>

<p>This course covers following topics:</p>

<ul>
  <li>
    <p>Design data processing systems</p>
  </li>
  <li>
    <p>Ensure solution quality</p>
  </li>
  <li>
    <p>Operationalize machine learning models</p>
  </li>
  <li>
    <p>Build and operationalize data processing systems</p>
  </li>
</ul>

<p>It has following 9 major classes with which you can attain a badge from google :</p>
<blockquote>
  <ol>
    <li>Google Cloud Big Data and Machine Learning Fundamentals</li>
    <li>Data Engineering on Google Cloud</li>
    <li>Serverless Data Processing with Dataflow Specialization</li>
    <li>Create and Manage Cloud Resources</li>
    <li>Perform Foundational Data, ML, and AI Tasks in Google Cloud</li>
    <li>Engineer Data in Google Cloud</li>
    <li>Preparing for the Google Cloud Professional Data Engineer Exam</li>
    <li>Professional Data Engineer</li>
    <li>Register for your certification exam</li>
  </ol>
</blockquote>

<p>I have followed the above courses using Partner account so it was completely free and there were free credits available to solve the lab.</p>

<ul>
  <li>
    <p><strong><em><a href="https://linuxacademy.com/course/google-cloud-data-engineer/">Linux Academy course</a></em></strong> (Took 4 weeks + 2 weeks (revision)) ‚Äî 75% success rate</p>
  </li>
  <li>
    <p><strong><em><a href="https://www.coursera.org/professional-certificates/gcp-data-engineering">Coursera GCP Data Engineering Specialization</a></em></strong> (6 courses)(Took ~4 Weeks) ‚Äî 20%</p>
  </li>
</ul>

<h3 id="3-books-and-resource">3. Books and resource</h3>

<p>I strongly recommend you to going through <strong><em><a href="https://www.oreilly.com/library/view/official-google-cloud/9781119618430/f07.xhtml#usec0005">Official Google Cloud Certified Professional Data Engineer Study Guide</a></em></strong> [Book] by Dan Sullivan. This study guide offers 100% coverage of every objective for the Google Cloud Certified Professional Data Engineer exam.</p>

<p>This books covers most of the exam topics and there are around 20 quizzes for each chapter.</p>

<h3 id="4-official-documentation">4. Official documentation</h3>

<p>It is also very important to going through the <strong><em><a href="https://cloud.google.com/">official documentation</a></em></strong> to get updated information about the google products and services.</p>

<p><em>For example,sometimes an old mock question‚Äôs answers will guide you in a wrong way. Access control on table level is now possible in BigQuery but it wasn‚Äôt before.</em> So please dont follow the quiz answers that were provided by default but also I recommend you to do your proper search in official docs , if you have doubt in it.I have used this scenerio as an example and there are other similar cases.</p>

<h3 id="5-quizzesexamtopicsmock-test">5. Quizzes/ExamTopics/Mock test</h3>

<p>Most important part of the preparation is going through as many quizzes as possible and verifing with reasonable answer. To pass the actual exam, you have to spend more time on learning &amp; re-learning through multiple practice tests.</p>

<p>I recommend the following sites for preparing exam:</p>

<ol>
  <li>
    <p><strong><em><a href="https://www.examtopics.com/exams/google/professional-data-engineer/">Examtopics</a></em></strong> : There will be robotic check once after 10 questions and we should pay to get rid of those check and which allow you to focus well in the preparation.Please be aware of wrong answers in Examtopics try to do a search by yourself to assure that its the correct one. Cost : <strong>FREE</strong></p>
  </li>
  <li>
    <p><strong><em><a href="https://www.passnexam.com/google/google-data-engineer/01">PassExam</a></em></strong>: I have also prepared with PassExam which provides right answers for most of the questions with a reason in description and related offical links. Cost : <strong>FREE</strong></p>
  </li>
  <li>
    <p><strong><em><a href="https://www.whizlabs.com/blog/google-cloud-professional-data-engineer-exam-questions/">WhizLabs</a></em></strong> : There are around 25 multiple choice questions with what we can able to get insights on the certification and feel a little more confident. Cost : <strong>FREE</strong></p>
  </li>
  <li>
    <p><strong><em><a href="https://docs.google.com/forms/d/e/1FAIpQLSfkWEzBCP0wQ09ZuFm7G2_4qtkYbfmk_0getojdnPdCYmq37Q/viewform">Sample Questions fom Google</a></em></strong> : There are around 25 sample questions from google at the end of its official course.You can get the answer and explanation once you submit the form. But it is not guaranteed to help you pass the exam.</p>
  </li>
</ol>

<h3 id="6-cheatsheet">6. <strong><em>Cheatsheet</em></strong></h3>

<p>I recommend you to go through the <strong><em><a href="https://github.com/ml874/Data-Engineering-on-GCP-Cheatsheet/blob/master/data_engineering_on_GCP.pdf">Cheatsheet</a></em></strong> once. Cheatsheet is currently a 9-page reference Data Engineering on the Google Cloud Platform. It covers the data engineering lifecycle, machine learning, Google case studies, and GCP‚Äôs storage, compute, and big data products.</p>

<p>Going through only this cheatsheet is not guaranteed to help you pass.But it can help you with your revision and refreshing the topics that you have pepared over the period before.</p>

<h3 id="7-notes">7. Notes</h3>

<p>Note taking app : [<strong>Notion](https://www.notion.so)</strong>. The best app for note taking.</p>

<p>Notion has helped us to <strong>revise effectively before exams on all the essential topics.</strong>I recommend to take notes for following topics.</p>

<p>These are the core GCP products that the assessment covers.</p>

<ul>
  <li>
    <p><a href="https://cloud.google.com/ai-platform/docs/technical-overview">AI Platform</a> (+ general AI and ML concepts)</p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/bigquery">BigQuery</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/bigtable">BigTable</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/sql">Cloud SQL</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/dataflow">Dataflow</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/dataproc">Dataproc</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/pubsub">Pub/Sub</a></p>
  </li>
</ul>

<p>But there was some overlap with other Google tools too.</p>

<ul>
  <li>
    <p><a href="https://cloud.google.com/storage">Cloud Storage</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/spanner">Cloud Spanner</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/composer">Composer</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/compute">Compute Engine</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/dlp">Data Loss Prevention</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/dataprep">Dataprep</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/datastore">Datastore</a></p>
  </li>
  <li>
    <p><a href="https://firebase.google.com/docs/firestore">Firestore</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/natural-language">Natural Language AI</a></p>
  </li>
  <li>
    <p><a href="https://www.tensorflow.org/">Tensorflow</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/text-to-speech">Text-to-speech</a></p>
  </li>
  <li>
    <p><a href="https://cloud.google.com/video-intelligence">Video AI</a></p>
  </li>
</ul>

<h3 id="8-tips">8. Tips</h3>

<ul>
  <li>
    <p>Preparation is key to success. so many weekends, many holidays, I spent preparing for this exam.</p>
  </li>
  <li>
    <p>You can eliminate all the answer that recommend <strong>non-GCP solutions</strong>.</p>
  </li>
  <li>
    <p>As with many multiple-choice exams, <strong>eliminate</strong>. If you can‚Äôt eliminate to one possible answer, make a guess.</p>
  </li>
</ul>

<h3 id="9-assessment">9. Assessment</h3>

<p>I took the assessment via Kryterion‚Äôs <a href="https://www.webassessor.com/googlecloud/">Webassessor</a>. <strong>Exam Duration</strong>: 2 hours, <strong>Registration fee</strong>: $200 (plus tax where applicable) <strong>, Languages</strong>: English, Japanese and <strong>Exam format</strong>: Multiple choice and multiple select taken remotely or in person at a test center.</p>

<p>On exam day, I had to show my surroundings via webcam, turn off my phone, empty my desk, If your name is not visible you should take photo from your mobile a,d then you should show it to the sponser staff at the other end etc. All this took ~ half an hour. I didn‚Äôt have to talk to the person behind the camera, but we can chat. The person also monitor you via webcam during the assessment.</p>

<p>I found the questions are bit hard and challenging. In the end, I got a provisional <em>Pass</em>, but it should be confirmed by Google ‚Äî for whatever reason. That confirmation came 4 ‚Äì10 days later. So now I am officially Google Certified Professional Data Engineer üéâ and here is my Official Google Certification!</p>

<p><img src="https://cdn-images-1.medium.com/max/2548/1*73NRxlyIZ8OH_2-CSYK4dQ.png" alt="Image by Author on [Professional Data Engineer Certification](https://www.credential.net/4e184ed8-23eb-4783-810c-a3c3e4e10b7d)" /></p>

<p>Tada üéâ</p>

<p><img src="https://cdn-images-1.medium.com/max/2214/1*-kDC5Id6bejn-wY7aRlHAw.jpeg" alt="Image by Author" /></p>

<p>Got this amazing <strong>Google hoodie</strong> for free from google, after a month from the date of ordering. You can order it via the link that they send you with your official confirmation mail.</p>

<h2 id="colclusion">Colclusion:</h2>

<p>I hope this will help you pass the exam. All in all, you need to have solid Google Cloud knowledge to start with. If you are not familiar with the technology, the advised courses will help you. To get well prepared for the exam, I encourage you to complete the Official Data Engineer course videos and read about the best practices of GCP products, followed by the ML Crash Course provided by Google or Coursera. You should be ready to pass the exam by combining your studies with your knowledge. Good luck!</p>
<blockquote>
  <h1 id="success-is-easy-to-achieve-once-you-set-your-mind-on-a-specific-goal">Success is easy to achieve once you set your mind on a specific goal.</h1>
  <h1 id="--atticus-aristotle">- Atticus Aristotle</h1>
</blockquote>

<p><strong>Wish you the very best with your GCP certifications. You can reach me via:</strong></p>

<blockquote>
  <p><strong>Connect with me on <a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Find me on <a href="https://github.com/Abonia1">Github</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Visit my technical channel on <a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Support: <a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Learn about my journey of preparing for and passing the Google Cloud Certified Professional Data Engineer exam.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/gcp-exam.webp" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/gcp-exam.webp" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deploying a RAG Application in AWS Lambda using Docker and ECR</title><link href="http://localhost:4000/blogs/2024-03-05-Deploying-a-RAG-Application-in-AWS-Lambda-using-Docker-and-ECR/" rel="alternate" type="text/html" title="Deploying a RAG Application in AWS Lambda using Docker and ECR" /><published>2024-03-05T00:00:00+01:00</published><updated>2024-04-12T10:28:22+02:00</updated><id>http://localhost:4000/blogs/Deploying-a-RAG-Application-in-AWS-Lambda-using-Docker-and-ECR</id><content type="html" xml:base="http://localhost:4000/blogs/2024-03-05-Deploying-a-RAG-Application-in-AWS-Lambda-using-Docker-and-ECR/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [AWS, Lambda , ECR ,Docker , LangChain, OpenAI]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#lambda--ecr--docker--langchain--openai" id="markdown-toc-lambda--ecr--docker--langchain--openai">Lambda ‚Äî ECR ‚Äî Docker ‚Äî LangChain ‚Äî OpenAI</a></li>
  <li><a href="#services-overview" id="markdown-toc-services-overview">Services Overview</a></li>
  <li><a href="#integration-and-deployment-steps" id="markdown-toc-integration-and-deployment-steps">Integration and Deployment Steps</a></li>
  <li><a href="#implementation" id="markdown-toc-implementation">Implementation</a>    <ul>
      <li><a href="#environment-setup" id="markdown-toc-environment-setup">Environment Setup</a></li>
      <li><a href="#data-loading-and-processing" id="markdown-toc-data-loading-and-processing">Data Loading and Processing</a></li>
      <li><a href="#response-generation" id="markdown-toc-response-generation">Response Generation</a></li>
      <li><a href="#aws-lambda-handler" id="markdown-toc-aws-lambda-handler">AWS Lambda Handler</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#thanks-for-reading" id="markdown-toc-thanks-for-reading"><em>Thanks for Reading!</em></a></li>
</ul>

<h3 id="lambda--ecr--docker--langchain--openai">Lambda ‚Äî ECR ‚Äî Docker ‚Äî LangChain ‚Äî OpenAI</h3>

<p>Deploying a RAG (Retrieval-Augmented Generation) application in AWS Lambda using Docker and Amazon Elastic Container Registry (ECR) with LangChain involves several steps and services. This article will explain each service and how they work together in an integrated way, followed by the steps to deploy it.</p>

<p>Here is the link to the complete tutorial on <strong><a href="https://youtu.be/gicsb9p7uj4?si=9F2l6z1rNpkUOoIR">Deploying RAG in AWS</a></strong>.</p>

<p>Before proceeding further, I would like to kindly suggest that you take some time to read and watch the tutorial titled <strong><a href="https://medium.com/@abonia/build-and-deploy-llm-application-in-aws-cca46c662749">Build and Deploy LLM Application in AWS</a></strong>. This tutorial can provide you with a solid foundation on Lambda LLM application deployment.
<strong><a href="https://medium.com/@abonia/build-and-deploy-llm-application-in-aws-cca46c662749">Build and Deploy LLM Application in AWS</a></strong></p>

<h2 id="services-overview">Services Overview</h2>

<p><img src="https://cdn-images-1.medium.com/max/3840/1*KBp2fuAVMvBfpE10p4nrrQ.png" alt="Architecture Overview ‚Äî Image by Author" /></p>

<p><a href="https://aws.amazon.com/lambda/getting-started/?gclid=CjwKCAjwzN-vBhAkEiwAYiO7oF7Q4aEK3hKUA0HVPEe3wXKen_tVUDmxun4s5PjasxZ-deFf-j19vxoCovkQAvD_BwE&amp;trk=65546593-a75a-4317-b5dc-80218abfdb10&amp;sc_channel=ps&amp;s_kwcid=AL!4422!3!651542249938!e!!g!!aws%20lambda&amp;ef_id=CjwKCAjwzN-vBhAkEiwAYiO7oF7Q4aEK3hKUA0HVPEe3wXKen_tVUDmxun4s5PjasxZ-deFf-j19vxoCovkQAvD_BwE:G:s&amp;s_kwcid=AL!4422!3!651542249938!e!!g!!aws%20lambda!19835810591!150095231954">AWS Lambda</a>: A serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you. It allows you to run code without provisioning or managing servers.</p>

<ul>
  <li>
    <p><a href="https://aws.amazon.com/ecr/">Amazon ECR</a>: A fully-managed container registry that makes it easy for developers to store, manage, and deploy Docker container images. It‚Äôs integrated with Amazon ECS and AWS Fargate, simplifying your development to production workflow.</p>
  </li>
  <li><a href="https://www.docker.com/">Docker</a>: A platform that uses containerization to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package. This ensures that the application runs quickly and reliably on any server.
    <blockquote>
      <p>Dowload Link: <a href="https://www.docker.com/products/docker-desktop/">https://www.docker.com/products/docker-desktop/</a></p>
    </blockquote>
  </li>
  <li><a href="https://www.langchain.com/">LangChain</a>: A framework for building and deploying language models, providing tools for document loading, vector storage, embeddings, and more. It‚Äôs used here to facilitate the deployment of the RAG model.</li>
</ul>

<h2 id="integration-and-deployment-steps">Integration and Deployment Steps</h2>

<ol>
  <li><strong>Prepare Your Environment:</strong> Ensure you have the AWS CLI installed and configured, Docker installed, and Python 3.11 or later installed and configured. You‚Äôll also need an active AWS account with the necessary permissions.
    <blockquote>
      <p>Download Link: <a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</a></p>
    </blockquote>
  </li>
  <li>
    <p><strong>Create a Dockerfile:</strong> This file defines the environment in which your Lambda function will run. It starts from a base image provided by AWS (<em><code class="language-plaintext highlighter-rouge">public.ecr.aws/lambda/python:3.12</code></em>) and copies your application code and dependencies into the image. It also installs any necessary Python packages from a <code class="language-plaintext highlighter-rouge">requirements.txt</code> file as below:</p>

    <p>langchain_community
 boto3==1.34.37
 numpy
 langchain
 langchainhub
 langchain-openai
 chromadb
 bs4
 tiktoken
 openai</p>
  </li>
</ol>

<div class="language-docker highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c"># Use the AWS base image for Python 3.12</span>
    FROM public.ecr.aws/lambda/python:3.12
    
    <span class="c"># Install build-essential to get the C++ compiler and other necessary tools</span>
    RUN microdnf update -y &amp;&amp; microdnf install -y gcc-c++ make
    
    <span class="c"># Copy requirements.txt</span>
    COPY requirements.txt ${LAMBDA_TASK_ROOT}
    
    # Install the specified packages
    RUN pip install -r requirements.txt
    
    <span class="c"># Copy function code</span>
    COPY lambda_function.py ${LAMBDA_TASK_ROOT}
    
    # Set the permissions to make the file executable
    RUN chmod +x lambda_function.py
    
    <span class="c"># Set the CMD to your handler</span>
    CMD [ "lambda_function.lambda_handler" ]
</code></pre></div></div>

<ol>
  <li>
    <p><strong>Build and Push Your Docker Image to Amazon ECR:</strong> Use the AWS CLI to create a repository in ECR, build your Docker image, and push it to the repository. This step requires permissions to interact with ECR and S3.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  aws ecr create-repository - repository-name my-rag-lambda
  docker build <span class="nt">-t</span> my-test-lambda <span class="nb">.</span>
  docker tag my-rag-lambda:latest &lt;aws_account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/my-test-lambda:latest
  docker push &lt;aws_account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/my-test-lambda:latest
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Create a Lambda Function</strong>: In the AWS Lambda console, create a new function using the container image option. Specify the URI of the image in ECR as the image source.</p>
  </li>
  <li>
    <p><strong>Configure Your Lambda Function</strong>: Set the necessary environment variables, such as <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> for LangChain, and configure any other settings as needed.</p>
  </li>
  <li>
    <p><strong>Test Your Lambda Function</strong>: Invoke your Lambda function to test it. You can do this from the AWS Lambda console or using the AWS CLI. Ensure that the function is correctly processing inputs and generating outputs as expected.</p>
  </li>
</ol>

<h2 id="implementation">Implementation</h2>

<p>Below code is designed to deploy a Retrieval-Augmented Generation (RAG) model using AWS Lambda, Docker, and Amazon ECR, with LangChain for language model deployment.</p>

<h3 id="environment-setup">Environment Setup</h3>

<p>First, imports necessary libraries and sets up the environment:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import boto3
import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os
# Retrieve the OpenAI API key from environment variables
OPENAI_API_KEY = os.environ['OPENAI_API_KEY']
print(OPENAI_API_KEY)
</code></pre></div></div>

<ul>
  <li>boto3: The AWS SDK for Python, allowing Python developers to write software that makes use of services like Amazon S3, Amazon EC2, and others.</li>
  <li>bs4: Beautiful Soup, a library for pulling data out of HTML and XML files.</li>
  <li>LangChain: A framework for building and deploying language models, providing tools for document loading, vector storage, embeddings, and more.</li>
  <li>Environment Variables: The code retrieves the <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> from the environment variables, which is crucial for accessing OpenAI‚Äôs API.</li>
</ul>

<h3 id="data-loading-and-processing">Data Loading and Processing</h3>

<p>The <code class="language-plaintext highlighter-rouge">load_data</code> function is responsible for loading, chunking, and indexing the contents of a web page:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def load_data():
 loader = WebBaseLoader(
 web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
 bs_kwargs=dict(
 parse_only=bs4.SoupStrainer(
 class_=("post-content", "post-title", "post-header")
 )
 ),
 )
 docs = loader.load()
 text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
 splits = text_splitter.split_documents(docs)
 vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
 retriever = vectorstore.as_retriever()
 return retriever
</code></pre></div></div>

<ul>
  <li>WebBaseLoader: Loads documents from web paths, using Beautiful Soup to parse and extract specific elements.</li>
  <li>RecursiveCharacterTextSplitter: Splits documents into chunks based on character count and overlap.</li>
  <li>Chroma: Creates a vector store from the split documents, using OpenAI embeddings for vectorization.</li>
  <li>Retriever: A retriever object that can be used to retrieve relevant documents based on queries.</li>
</ul>

<h3 id="response-generation">Response Generation</h3>

<p>The <code class="language-plaintext highlighter-rouge">get_response</code> function generates a response to a given query using the RAG model:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_response(query):
 prompt = hub.pull("rlm/rag-prompt")
 llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
 retriever = load_data()
 rag_chain = (
 {"context": retriever | format_docs, "question": RunnablePassthrough()}
 | prompt
 | llm
 | StrOutputParser()
 )
 return rag_chain.invoke(query)
</code></pre></div></div>

<ul>
  <li>hub.pull: Retrieves a prompt from <a href="https://smith.langchain.com/hub/rlm/rag-prompt">LangChain‚Äôs hub</a>.</li>
  <li>ChatOpenAI: Initializes a chat model with GPT-3.5 Turbo.</li>
  <li>RunnablePassthrough: A component that passes the input directly to the next component in the chain.</li>
  <li>StrOutputParser: Parses the output of the RAG model into a string format.</li>
</ul>

<h3 id="aws-lambda-handler">AWS Lambda Handler</h3>

<p>Finally, the <code class="language-plaintext highlighter-rouge">lambda_handler</code> function is the entry point for AWS Lambda, which receives an event and context, processes the query, and returns a response:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def lambda_handler(event, context):
 query = event.get("question")
 response = get_response(query)
 print("response:", response)
 return {"body": response, "statusCode": 200}
</code></pre></div></div>

<ul>
  <li>
    <p>Event and Context: AWS Lambda passes an event object and a context object to the handler. The event object contains information about the triggering event, and the context object contains information about the runtime environment.</p>
  </li>
  <li>
    <p>Query Processing: The function extracts the query from the event, generates a response using the <code class="language-plaintext highlighter-rouge">get_response</code> function, and prints the response and returns a response object containing the generated response and a status code of 200, indicating success.</p>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Deploying a RAG model in AWS Lambda using Docker and ECR with LangChain involves preparing your environment, creating a Dockerfile, building and pushing your Docker image to ECR, creating a Lambda function, configuring it, and testing it. This process leverages the serverless capabilities of AWS Lambda, the containerization benefits of Docker, and the language model deployment capability provided by LangChain.</p>

<blockquote>
  <p><strong>Connect with me on <a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Find me on <a href="https://github.com/Abonia1">Github</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Visit my technical channel on <a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>
</blockquote>

<blockquote>
  <p><strong>Support: <a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>
</blockquote>

<blockquote>
  <h1 id="thanks-for-reading"><em>Thanks for Reading!</em></h1>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Deploying a RAG (Retrieval-Augmented Generation) application in AWS Lambda using Docker and Amazon Elastic Container Registry (ECR) with LangChain.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/aws-rag.webp" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/aws-rag.webp" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">EDA ‚Äî Visualize Embeddings of RAG</title><link href="http://localhost:4000/blogs/2024-01-05-EDA-Visualize-Embeddings-of-RAG/" rel="alternate" type="text/html" title="EDA ‚Äî Visualize Embeddings of RAG" /><published>2024-01-05T00:00:00+01:00</published><updated>2024-04-12T10:28:22+02:00</updated><id>http://localhost:4000/blogs/EDA%E2%80%94Visualize-Embeddings-of-RAG</id><content type="html" xml:base="http://localhost:4000/blogs/2024-01-05-EDA-Visualize-Embeddings-of-RAG/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [UMAP,RAG,Visualization,Langchain,ChromaHuggingFaceEmbeddings]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#introduction-to-rag" id="markdown-toc-introduction-to-rag">Introduction to RAG</a></li>
  <li><a href="#setting-up-the-environment" id="markdown-toc-setting-up-the-environment">Setting Up the Environment</a></li>
  <li><a href="#loading-the-model-and-vector-store" id="markdown-toc-loading-the-model-and-vector-store">Loading the Model and Vector Store</a></li>
  <li><a href="#fetching-and-preparing-data" id="markdown-toc-fetching-and-preparing-data">Fetching and Preparing Data</a></li>
  <li><a href="#calculating-distances" id="markdown-toc-calculating-distances">Calculating Distances</a></li>
  <li><a href="#visualizing-embeddings" id="markdown-toc-visualizing-embeddings">Visualizing Embeddings</a>    <ul>
      <li><a href="#1-using-spotlight" id="markdown-toc-1-using-spotlight">1. Using Spotlight</a></li>
      <li><a href="#2-using-umap" id="markdown-toc-2-using-umap">2. Using UMAP</a></li>
      <li><a href="#3-using-tensorboard" id="markdown-toc-3-using-tensorboard">3. Using Tensorboard</a></li>
      <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
    </ul>
  </li>
  <li><a href="#thanks-for-reading" id="markdown-toc-thanks-for-reading">Thanks for Reading!</a></li>
</ul>

<p>In this article, we delve into the how we can visualize Retrieval Augmented Generation (RAG) data using the langchain framework in conjunction with Hugging Face‚Äôs language models and embeddings. We‚Äôll explore how to leverage the <code class="language-plaintext highlighter-rouge">HuggingFaceEmbeddings</code> and <code class="language-plaintext highlighter-rouge">Chroma</code> vector store for efficient embedding and document retrieval, followed by a practical example of visualizing these embeddings to understand their distribution and relevance to a given question.</p>

<p>We‚Äôll explore how to use the Hugging Face Embeddings for embedding text data and store it in chroma and then visualize it using UMAP (Uniform Manifold Approximation and Projection), a dimensionality reduction technique that helps in visualizing high-dimensional data in a more interpretable way.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://cdn-images-1.medium.com/max/2924/1*5nX7pJ5xOoMelSGm67hhHA.gif" alt="Courtesy of Spotlight" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><i>Image Credits - Courtesy of Spotlight</i></td>
    </tr>
  </tbody>
</table>

<h3 id="introduction-to-rag">Introduction to RAG</h3>

<p>RAG is a cutting-edge approach that combines the strengths of pretrained Large Language Models (LLMs) and your own data to generate responses. It retrieves documents, passes them through a sequence-to-sequence model, and then marginalizes to generate outputs. This method is particularly useful for querying specific documents or interacting with your own data in a conversational manner .</p>

<h3 id="setting-up-the-environment">Setting Up the Environment</h3>

<p>To begin, we‚Äôll need to import the necessary libraries and set up our environment. We‚Äôll use <code class="language-plaintext highlighter-rouge">pandas</code> for data manipulation, <code class="language-plaintext highlighter-rouge">langchain.embeddings</code> for handling embeddings, and <code class="language-plaintext highlighter-rouge">langchain.vectorstores</code> for our vector store.
<a href="https://python.langchain.com/docs/get_started/introduction"><strong>Introduction | ü¶úÔ∏èüîó Langchain</strong>
<em>LangChain is a framework for developing applications powered by language models. It enables applications that:</em>python.langchain.com</a></p>

<p>So do install as follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!pip install pandas langchain renumics-spotlight umap-learn

import pandas as pd
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
</code></pre></div></div>

<h3 id="loading-the-model-and-vector-store">Loading the Model and Vector Store</h3>

<p>Next, we‚Äôll specify the model path for our embeddings and create instances of <code class="language-plaintext highlighter-rouge">HuggingFaceEmbeddings</code> and <code class="language-plaintext highlighter-rouge">Chroma</code>. We‚Äôll also configure the model to use the CPU/GPU for computations and ensure embeddings are not normalized for our visualization purposes.</p>

<p>Before dive in we consider, you already have chroma db collection and see how we can visualize it.If no you can create one as follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader
from bs4 import BeautifulSoup as Soup
from langchain.vectorstores import Chroma

url = "https://www.freenews.fr/"
loader = RecursiveUrlLoader(
    url=url, max_depth=5, extractor=lambda x: Soup(x, "lxml").text
)
documents = loader.load()
text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=0
        )
texts = text_splitter.split_documents(documents)

modelPath = "sentence-transformers/distiluse-base-multilingual-cased-v1"

model_kwargs = {'device':'cpu'}
#encode_kwargs = {'normalize_embeddings': False}

embeddings = HuggingFaceEmbeddings(model_name=modelPath,model_kwargs=model_kwargs) 

# SAVE
docs_vectorstore = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db_multilingual")
</code></pre></div></div>

<p>Provide the link to your chroma persist_directory below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>modelPath = "sentence-transformers/distiluse-base-multilingual-cased-v1"
model_kwargs = {'device':'cpu'}
encode_kwargs = {'normalize_embeddings': False}
embeddings_model = HuggingFaceEmbeddings(model_name=modelPath, model_kwargs=model_kwargs)
docs_vectorstore = Chroma(persist_directory="./chroma_db_multilingual", embedding_function=embeddings_model)
</code></pre></div></div>

<h3 id="fetching-and-preparing-data">Fetching and Preparing Data</h3>

<p>We‚Äôll retrieve our data from the vector store, including metadata, documents, and embeddings. Then, we‚Äôll create a DataFrame to organize this data and add a column to indicate whether each document contains the answer to our query.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>response = docs_vectorstore.get(include=["metadatas", "documents", "embeddings"])
df = pd.DataFrame({
 "id": response["ids"],
 "source": [metadata.get("source") for metadata in response["metadatas"]],
 "page": [metadata.get("page", -1) for metadata in response["metadatas"]],
 "document": response["documents"],
 "embedding": response["embeddings"],
})
df["contains_answer"] = df["document"].apply(lambda x: "N≈ìud R√©partition Optique)" in x)
</code></pre></div></div>

<h2 id="calculating-distances">Calculating Distances</h2>

<p>To find the closest match to our question, we calculate the Euclidean distance between the question embedding and each document embedding.</p>

<p><img src="https://cdn-images-1.medium.com/max/4408/1*68cE3DuXYssA7tAvtYm7-Q.png" alt="Euclidean Distance" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>question_embedding = embeddings_model.embed_query(question)
df["dist"] = df.apply(
    lambda row: np.linalg.norm(
        np.array(row["embedding"]) - question_embedding
    ),
    axis=1,
)
</code></pre></div></div>

<h2 id="visualizing-embeddings">Visualizing Embeddings</h2>

<h3 id="1-using-spotlight">1. Using Spotlight</h3>

<p>This visualization will help us understand how closely related documents are to our query and identify potential areas of improvement or further investigation.</p>

<p>Finally, we use the spotlight module from renumics to visualize the data. This step is crucial for understanding the distribution of documents in relation to the question and the model‚Äôs response.
<strong><a href="https://github.com/Renumics/spotlight">GitHub - Interactively explore unstructured datasets from your dataframe-Interactively explore unstructured datasets from your dataframe</a></strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from renumics import spotlight
spotlight.show(df)
</code></pre></div></div>

<p><img src="https://cdn-images-1.medium.com/v2/resize:fit:6776/1*jOH05LoRaN3TnJwiB9V1Dg.png" alt="UMAP ‚Äî Visualize the embeddings" /></p>

<p><strong><a href="https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial.ipynb">Notebook-Renumics/rag-demo.Retrieval-Augmented Generation Assistant Demo</a></strong></p>

<h3 id="2-using-umap">2. Using UMAP</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import umap
# Find the  5 closest vectors
closest_vectors_indices = df.nsmallest(5, 'dist')['id'].values

# Prepare the embeddings for UMAP
embeddings = np.array([np.array(x) for x in df["embedding"]])

# Reduce dimensionality with UMAP
reducer = umap.UMAP()
embedding_reduced = reducer.fit_transform(embeddings)

# Plot the reduced embeddings
plt.scatter(embedding_reduced[:,  0], embedding_reduced[:,  1], c='gray', alpha=0.2)

# Highlight the question embedding and the  5 closest vectors
plt.scatter(embedding_reduced[df["id"].isin(closest_vectors_indices),  0], embedding_reduced[df["id"].isin(closest_vectors_indices),  1], c='red', alpha=1)
plt.scatter(embedding_reduced[df["id"] == df[df["dist"] == df["dist"].min()]["id"].values[0],  0], embedding_reduced[df["id"] == df[df["dist"] == df["dist"].min()]["id"].values[0],  1], c='blue', alpha=1, marker='*')

# Add labels and title
plt.title("UMAP Visualization of Text Embeddings with Question Highlighted")
plt.xlabel("UMAP  1")
plt.ylabel("UMAP  2")
plt.show()
</code></pre></div></div>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*5q-fn2cWbzzJP6ielmyPxA.png" alt="UMAP ‚Äî Visualize Text embedding ‚Äî Blue point is the Question" /></p>

<h3 id="3-using-tensorboard">3. Using Tensorboard</h3>

<p>Another effective way to visualize embeddings, especially useful for gaining insights into word embeddings and the relationships between them, is by using TensorBoard. TensorBoard is a tool that allows for the visualization of machine learning models and their metrics, including word embeddings, in a user-friendly interface. It can be particularly beneficial when you want to explore the semantic similarities and relationships between words in your embeddings.</p>

<p><strong>Setting Up TensorBoard</strong></p>

<p>First, ensure you have TensorBoard installed. You can install it using pip:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install tensorboard
</code></pre></div></div>

<p><strong>Visualizing Word Embeddings with TensorBoard</strong></p>

<p>To visualize word embeddings using TensorBoard, you‚Äôll typically need to convert your embeddings into a format that TensorBoard can understand. This often involves creating a metadata file that maps words to their embeddings and then using TensorBoard‚Äôs <code class="language-plaintext highlighter-rouge">Projector</code> to visualize these embeddings.
<strong><a href="https://www.tensorflow.org/tensorboard/get_started">Get started with TensorBoard | TensorFlow</a></strong></p>

<p>Here‚Äôs a simplified step-by-step guide on how to do this:</p>

<ol>
  <li>
    <p>Prepare Your Embeddings: Ensure your embeddings are in a suitable format. For TensorBoard, you might need to convert your embeddings into a <code class="language-plaintext highlighter-rouge">.tsv</code> (Tab-Separated Values) file where each line contains a word and its corresponding embedding vector.</p>
  </li>
  <li>
    <p>Create a Metadata File: Alongside your embeddings file, create a metadata file that maps each word to its index in the embeddings file. This file is also in <code class="language-plaintext highlighter-rouge">.tsv</code> format.</p>
  </li>
  <li>
    <p>Use TensorBoard‚Äôs Projector:
 ‚Äî Start TensorBoard by running <code class="language-plaintext highlighter-rouge">tensorboard ‚Äî logdir=path/to/your/logs</code> in your terminal.
 ‚Äî In your web browser, navigate to the TensorBoard interface (usually at <code class="language-plaintext highlighter-rouge">localhost:6006</code>).
 ‚Äî Go to the <code class="language-plaintext highlighter-rouge">Projector</code> tab.
 ‚Äî Click on <code class="language-plaintext highlighter-rouge">Load</code> under the <code class="language-plaintext highlighter-rouge">Embeddings</code> section.
 ‚Äî Upload your embeddings file and metadata file.</p>
  </li>
  <li>
    <p>Explore Your Embeddings:
 ‚Äî Once your embeddings are loaded, you can explore them in various ways, such as through a 2D or 3D scatter plot, where each point represents a word and its position is determined by its embedding vector.
 ‚Äî You can also use the <code class="language-plaintext highlighter-rouge">Word</code> search bar to find specific words and see how they are positioned relative to others in the embedding space.</p>
  </li>
</ol>

<p><img src="https://cdn-images-1.medium.com/max/5200/0*Bk4Bf3VD8P2FKl9M" alt="" /></p>

<p>-Semantic Similarity: TensorBoard makes it easier to visually inspect the semantic relationships between words by examining their positions in the embedding space.</p>
<ul>
  <li>Ease of Use: The user-friendly interface of TensorBoard allows for intuitive exploration of embeddings without the need for complex code.</li>
  <li>Insight into Model Performance: Beyond visualizing embeddings, TensorBoard can also be used to track model metrics, network weight distributions, and other performance indicators, providing a comprehensive toolkit for monitoring and understanding your models.</li>
</ul>

<h3 id="conclusion">Conclusion</h3>

<p>Visualizing RAG data provides valuable insights into the model‚Äôs decision-making process, helping us understand how it selects and interprets information from external documents.By visualizing RAG data, we gain valuable insights into the relationships between documents and our queries. This technique not only aids in understanding the performance of our RAG system but also guides us in refining models and datasets for better accuracy and relevance.</p>

<blockquote>
  <h1 id="thanks-for-reading">Thanks for Reading!</h1>
</blockquote>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Visualize Retrieval Augmented Generation (RAG) data using the langchain framework in conjunction with Hugging Face‚Äôs language models and embeddings and chromaDB.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/EDA-RAG.GIF" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/EDA-RAG.GIF" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Best LLM and LLMOps Resources for 2023</title><link href="http://localhost:4000/blogs/2023-12-05-Best-LLM-and-LLMOps-Resources-for-2023/" rel="alternate" type="text/html" title="Best LLM and LLMOps Resources for 2023" /><published>2023-12-05T00:00:00+01:00</published><updated>2024-04-12T10:28:22+02:00</updated><id>http://localhost:4000/blogs/Best-LLM-and-LLMOps-Resources-for-2023</id><content type="html" xml:base="http://localhost:4000/blogs/2023-12-05-Best-LLM-and-LLMOps-Resources-for-2023/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [Large Language Models, GPT-3, ChatGPT, Books, Courses And Training]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#ùêÅùêûùê¨ùê≠-ùêúùê®ùêÆùê´ùê¨ùêûùê¨-ùêõùê®ùê®ùê§ùê¨-ùêöùêßùêù-ùê´ùêûùê¨ùê®ùêÆùê´ùêúùêûùê¨-ùê≠ùê®-ùê¨ùê≠ùêöùê´ùê≠ùêöùêùùêØùêöùêßùêúùêû-ùê≤ùê®ùêÆùê´-ùêãùêãùêå-ùêöùêßùêù-ùêãùêãùêåùêéùê©ùê¨-ùê£ùê®ùêÆùê´ùêßùêûùê≤" id="markdown-toc-ùêÅùêûùê¨ùê≠-ùêúùê®ùêÆùê´ùê¨ùêûùê¨-ùêõùê®ùê®ùê§ùê¨-ùêöùêßùêù-ùê´ùêûùê¨ùê®ùêÆùê´ùêúùêûùê¨-ùê≠ùê®-ùê¨ùê≠ùêöùê´ùê≠ùêöùêùùêØùêöùêßùêúùêû-ùê≤ùê®ùêÆùê´-ùêãùêãùêå-ùêöùêßùêù-ùêãùêãùêåùêéùê©ùê¨-ùê£ùê®ùêÆùê´ùêßùêûùê≤">ùêÅùêûùê¨ùê≠ ùêúùê®ùêÆùê´ùê¨ùêûùê¨, ùêõùê®ùê®ùê§ùê¨ ùêöùêßùêù ùê´ùêûùê¨ùê®ùêÆùê´ùêúùêûùê¨ ùê≠ùê® ùê¨ùê≠ùêöùê´ùê≠/ùêöùêùùêØùêöùêßùêúùêû ùê≤ùê®ùêÆùê´ ùêãùêãùêå ùêöùêßùêù ùêãùêãùêåùêéùê©ùê¨ ùê£ùê®ùêÆùê´ùêßùêûùê≤:</a></li>
  <li><a href="#-courses" id="markdown-toc--courses">üë©‚Äçüè´ Courses</a></li>
  <li><a href="#-curated-list" id="markdown-toc--curated-list">üí• Curated List</a></li>
  <li><a href="#-top-llm-books" id="markdown-toc--top-llm-books">üìö Top LLM Books</a></li>
  <li><a href="#-llm-reading-list" id="markdown-toc--llm-reading-list">üìñ LLM Reading List</a>    <ul>
      <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion:</a></li>
    </ul>
  </li>
</ul>

<p><img src="https://cdn-images-1.medium.com/max/2048/1*JeK2nweJRazyOThkJzT_HQ.gif" alt="Image by Author" />
<a href="https://github.com/Abonia1/deeplearningai-generativeAI-short-courses/tree/main"><strong>GitHub - Abonia1/deeplearningai-generativeAI-short-courses: Notebook samples - Generative AI short‚Ä¶</strong>
<em>Notebook samples - Generative AI short courses - deeplearning.ai - Abonia1/deeplearningai-generativeAI-short-courses</em>github.com</a></p>

<h3 id="ùêÅùêûùê¨ùê≠-ùêúùê®ùêÆùê´ùê¨ùêûùê¨-ùêõùê®ùê®ùê§ùê¨-ùêöùêßùêù-ùê´ùêûùê¨ùê®ùêÆùê´ùêúùêûùê¨-ùê≠ùê®-ùê¨ùê≠ùêöùê´ùê≠ùêöùêùùêØùêöùêßùêúùêû-ùê≤ùê®ùêÆùê´-ùêãùêãùêå-ùêöùêßùêù-ùêãùêãùêåùêéùê©ùê¨-ùê£ùê®ùêÆùê´ùêßùêûùê≤">ùêÅùêûùê¨ùê≠ ùêúùê®ùêÆùê´ùê¨ùêûùê¨, ùêõùê®ùê®ùê§ùê¨ ùêöùêßùêù ùê´ùêûùê¨ùê®ùêÆùê´ùêúùêûùê¨ ùê≠ùê® ùê¨ùê≠ùêöùê´ùê≠/ùêöùêùùêØùêöùêßùêúùêû ùê≤ùê®ùêÆùê´ ùêãùêãùêå ùêöùêßùêù ùêãùêãùêåùêéùê©ùê¨ ùê£ùê®ùêÆùê´ùêßùêûùê≤:</h3>

<h2 id="-courses">üë©‚Äçüè´ Courses</h2>

<p>üëâ Generative AI learning path: <a href="https://www.cloudskillsboost.google/paths/118">https://www.cloudskillsboost.google/paths/118</a></p>

<p>üëâ Full Stack LLM Bootcamp : <a href="https://lnkd.in/eH8VXpwB">https://lnkd.in/eH8VXpwB</a></p>

<p>üëâ LLM University to learn about LLMs and NLP ‚Äî Cohere : <a href="https://lnkd.in/etKAjaYg">https://lnkd.in/etKAjaYg</a></p>

<p>üëâ Deploying GPT and Large Language Models ‚Äî Oreilly : <a href="https://lnkd.in/eDDivjB6">https://lnkd.in/eDDivjB6</a></p>

<p>üëâ Professional Certificate in Large Language Models- edX : <a href="https://lnkd.in/edg3gPzQ">https://lnkd.in/edg3gPzQ</a></p>

<p>üëâ Understanding Large Language Models ‚Äî Princeton University : <a href="https://lnkd.in/eE44cmza">https://lnkd.in/eE44cmza</a></p>

<p>üëâ Natural Language Processing Specialization ‚Äî coursera : <a href="https://lnkd.in/eNGDYGeA">https://lnkd.in/eNGDYGeA</a></p>

<p>üëâ Large Language Models ‚Äî The Stanford CS324 : <a href="https://lnkd.in/eJKfDTHK">https://lnkd.in/eJKfDTHK</a></p>

<p>üëâ Transformers Course ‚Äî HuggingFace : <a href="https://lnkd.in/eY2-NdGG">https://lnkd.in/eY2-NdGG</a></p>

<p>üëâ Large Language Models ‚Äî Class Central : <a href="https://lnkd.in/exVh6g-K">https://lnkd.in/exVh6g-K</a></p>

<p>üëâ Large Language Models ‚Äî Rycolab : <a href="https://lnkd.in/eRR_EzGW">https://lnkd.in/eRR_EzGW</a></p>

<h2 id="-curated-list">üí• Curated List</h2>

<p>Elevate your Generative AI expertise with a carefully curated selection of Andrew Ng‚Äôs GenAI courses. Expand your skill set through focused modules offered by <strong><a href="http://deeplearning.ai/">DeepLearning.AI</a> ‚Äî <a href="https://www.deeplearning.ai/short-courses/">https://www.deeplearning.ai/short-courses/</a></strong></p>

<p>üëâ ChatGPT Prompt Engineering for Developers:
Go beyond the chat box. Use API access to leverage LLMs into your own applications, and learn to build a custom chatbot.
Level : Beginner to Advanced</p>

<p>üìå Link : <a href="https://lnkd.in/gWZUEK2A">https://lnkd.in/gWZUEK2A</a></p>

<p>üëâ Building Systems with the ChatGPT API
Level up your use of LLMs. Learn to break down complex tasks, automate workflows, chain LLM calls, and get better outputs.</p>

<p>Level : Beginner to Advanced</p>

<p>üìå Link : <a href="https://lnkd.in/gq9kjmQf">https://lnkd.in/gq9kjmQf</a></p>

<p>üëâ LangChain for LLM Application Development
The framework to take LLMs out of the box. Learn to use LangChain to call LLMs into new environments, and use memories, chains, and agents to take on new and complex tasks.</p>

<p>Level : Beginner</p>

<p>üìå Link: <a href="https://lnkd.in/ggpgxHm7">https://lnkd.in/ggpgxHm7</a></p>

<p>üëâ LangChain: Chat with Your Data
Create a chatbot to interface with your private data and documents using LangChain.</p>

<p>Level : Beginner</p>

<p>üìå Link : <a href="https://lnkd.in/gUZrfksz">https://lnkd.in/gUZrfksz</a></p>

<p>üëâ Building Generative AI Applications with Gradio
Create and demo machine learning applications quickly. Share your app with the world on Hugging Face Spaces.</p>

<p>Level : Beginner</p>

<p>üìå Link : <a href="https://lnkd.in/gCHRS7nv">https://lnkd.in/gCHRS7nv</a></p>

<p>üëâ Evaluating and Debugging Generative AI
Learn MLOps tools for managing, versioning, debugging and experimenting in your ML workflow.</p>

<p>Level : Intermediate</p>

<p>üìå Link : <a href="https://lnkd.in/gdZd-prA">https://lnkd.in/gdZd-prA</a></p>

<p>üëâ How Diffusion Models Work
Learn and build diffusion models from the ground up. Start with an image of pure noise, and arrive at a final image, learning and building intuition at each step along the way.</p>

<p>Level : Intermediate</p>

<p>üìå Link : <a href="https://lnkd.in/g7ajmY4X">https://lnkd.in/g7ajmY4X</a></p>

<p>üëâ Finetuning Large Language Models
Learn to finetune an LLM in minutes and specialize it to use your own data</p>

<p>Level : Intermediate</p>

<p>üìå Link : <a href="https://lnkd.in/ghxMEpCX">https://lnkd.in/ghxMEpCX</a></p>

<p>üëâ Functions, Tools and Agents with LangChain
Learn about the most recent advancements in LLM APIs.Use LangChain Expression Language (LCEL), a new syntax to compose and customize chains and agents faster.</p>

<p>Level : Intermediate</p>

<p>üìå Link : <a href="https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/">https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/</a></p>

<p>üëâ LLMOps
Learn LLMOps best practices as you design and automate the steps to tune an LLM for a specific task and deploy it as a callable API. In the course, you‚Äôll tune an LLM to act as a question-answering coding expert.</p>

<p>Level : Beginner</p>

<p>üìå Link : <a href="https://learn.deeplearning.ai/courses/llmops/lesson/1/introduction">https://learn.deeplearning.ai/courses/llmops/lesson/1/introduction</a></p>

<p>üëâ Advanced Retrieval for AI with Chroma
Learn advanced retrieval techniques to improve the relevancy of retrieved results.</p>

<p>Level : Intermediate</p>

<p>üìå Link : <a href="https://learn.deeplearning.ai/courses/advanced-retrieval-for-ai/lesson/1/introduction">https://learn.deeplearning.ai/courses/llmops/lesson/1/introduction</a></p>

<h2 id="-top-llm-books">üìö Top LLM Books</h2>

<p><img src="https://cdn-images-1.medium.com/max/3368/1*K12j0-zJ-wELcW0gtPWYRQ.png" alt="" /></p>

<p><img src="https://cdn-images-1.medium.com/max/3332/1*AdXyZe1YTMoAQeM1FZEpQg.png" alt="" /></p>

<p>üëâ Practical Natural Language Processing ‚Äî Oreilly : <a href="https://lnkd.in/eKBHvdzM">https://lnkd.in/eKBHvdzM</a></p>

<p>üëâ Natural Language Processing with Transformers ‚Äî Oreilly : <a href="https://lnkd.in/ehazWcMY">https://lnkd.in/ehazWcMY</a></p>

<p>üëâ Transformers for Natural Language Processing ‚Äî Packt : <a href="https://lnkd.in/e_Y5cX6c">https://lnkd.in/e_Y5cX6c</a></p>

<p>üëâ GPT-3: Building Innovative NLP Products Using Large Language Models : <a href="https://lnkd.in/eSpvDErp">https://lnkd.in/eSpvDErp</a></p>

<p>üëâ Hands-On Generative AI with Transformers and Diffusion Models: <a href="https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/">https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/</a></p>

<p>üëâ Quick Start Guide to Large Language Models: Strategies and Best Practices for using ChatGPT and Other LLMs : <a href="https://lnkd.in/erbhdEjU">https://lnkd.in/erbhdEjU</a></p>

<h2 id="-llm-reading-list">üìñ LLM Reading List</h2>

<p>üëâ Understanding Large Language Models-A Transformative Reading List : <a href="https://lnkd.in/eEv2vi2w">https://lnkd.in/eEv2vi2w</a></p>

<p>üëâ Practical Deep Learning for Coders : <a href="https://course.fast.ai/">https://course.fast.ai</a></p>

<p>üëâ The Annotated Transformer : <a href="https://lnkd.in/et883Grd">https://lnkd.in/et883Grd</a></p>

<p>üëâ The Illustrated Transformer : <a href="https://lnkd.in/ehc9Bpk7">https://lnkd.in/ehc9Bpk7</a></p>

<p>üëâ Langchain Demo : <a href="https://lnkd.in/eWyWzZrb">https://lnkd.in/eWyWzZrb</a></p>

<table>
  <tbody>
    <tr>
      <td>üëâ State of GPT</td>
      <td>BRK216HFS : <a href="https://www.youtube.com/watch?v=bZQun8Y4L2A&amp;themeRefresh=1">https://www.youtube.com/watch?v=bZQun8Y4L2A&amp;themeRefresh=1</a></td>
    </tr>
  </tbody>
</table>

<p>üëâ Awsome LLMOps ‚Äî GitHub : <a href="https://lnkd.in/e3KNspKi">https://lnkd.in/e3KNspKi</a></p>

<p>üëâ Awsome LLM Repo ‚Äî GitHub : <a href="https://lnkd.in/eR2JwbPV">https://lnkd.in/eR2JwbPV</a></p>

<p>üëâ LLM Cheatsheet ‚Äî Github : <a href="https://github.com/Abonia1/CheatSheet-LLM">https://github.com/Abonia1/CheatSheet-LLM</a></p>

<p>üëâ LLMOps ‚Äî <a href="https://vinija.ai/concepts/LLMOps/">https://vinija.ai/concepts/LLMOps/</a></p>

<p>üëâ OpenAI Cookbook ‚Äî <a href="https://github.com/openai/openai-cookbook">https://github.com/openai/openai-cookbook</a></p>

<h3 id="conclusion">Conclusion:</h3>

<p>I hope you found this compilation of resources on large language models to be beneficial. We have included a variety of courses, books, reading lists, and other valuable resources and frameworks that can assist you in constructing your own impactful applications based on LLMs. Stay tune for another intresting article on LLM.</p>

<p>Keep an eye out for future updates to this list, and stay tuned for another intriguing article on LLM.</p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Curated list of best courses, books, resources on large language model]]></summary></entry><entry><title type="html">YOLOv8 Fire and Smoke Detection</title><link href="http://localhost:4000/blogs/2023-02-10-YOLOv8-Fire-and-Smoke-Detection/" rel="alternate" type="text/html" title="YOLOv8 Fire and Smoke Detection" /><published>2023-02-10T00:00:00+01:00</published><updated>2024-04-15T19:07:47+02:00</updated><id>http://localhost:4000/blogs/YOLOv8-Fire-and-Smoke-Detection</id><content type="html" xml:base="http://localhost:4000/blogs/2023-02-10-YOLOv8-Fire-and-Smoke-Detection/"><![CDATA[<p><strong>Reading_time:</strong> 6 min<br />
  <strong>Tags:</strong> [Yolov8, Fireandsmokedetection, Machine Learning, Computer Vision, AI]</p>
<ul class="large-only" id="markdown-toc">
  <li><a href="#yolov8" id="markdown-toc-yolov8">YOLOv8</a></li>
  <li><a href="#dataset" id="markdown-toc-dataset">Dataset</a></li>
  <li><a href="#model-training-and-evaluation" id="markdown-toc-model-training-and-evaluation">Model Training and Evaluation</a></li>
  <li><a href="#inference" id="markdown-toc-inference">Inference</a></li>
  <li><a href="#application-area" id="markdown-toc-application-area"><strong>Application Area</strong></a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>
<p><strong>Table of Contents:</strong></p>
<ul>
  <li><a href="#dataset">Dataset</a></li>
  <li><a href="#model-training-and-evaluation">Model Training and Evaluation</a></li>
  <li><a href="#Inference">Inference</a></li>
  <li><a href="#application-area">Application Area</a></li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://cdn-images-1.medium.com/max/2000/1*HVm63Me_kPRNy0vxekCabw.gif" alt="Media by Author - Live Fire and smoke detection and tracking using YOLOv8" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><i>Image Credits - Media by Author - Live Fire and smoke detection and tracking using YOLOv8</i></td>
    </tr>
  </tbody>
</table>

<p>Fire and smoke detection is a critical task for ensuring public safety and preventing property damage. With recent advances in computer vision and deep learning, it is possible to build accurate fire and smoke detection systems using custom datasets. One such system is YOLOv8, a state-of-the-art object detection model that can be trained on a custom dataset to detect fire and smoke.</p>

<p>This article will redirect you to the <strong><a href="https://github.com/Abonia1/YOLOv8-Fire-and-Smoke-Detection">project</a></strong>.</p>

<p><strong><a href="https://github.com/Abonia1/YOLOv8-Fire-and-Smoke-Detection">This repository contains the code for tracking and detecting fires and smokes in real-time video using YOLOv8.</a></strong></p>

<h2 id="yolov8">YOLOv8</h2>

<p><strong><a href="https://github.com/ultralytics/ultralytics">Ultralytics YOLOv8</a></strong>, developed by <strong><a href="https://ultralytics.com">Ultralytics</a></strong>. YOLOv8 stands for ‚ÄúYou Only Look Once version 8‚Äù and it is an improvement over the previous YOLO models. It is a single-shot object detection model that can detect multiple objects in an image in real-time. Unlike other object detection models that divide the image into multiple regions and perform object detection on each region, YOLOv8 performs object detection on the entire image in one forward pass through the network. This makes it fast and efficient for real-time applications.</p>

<h2 id="dataset">Dataset</h2>

<p>We have used image dataset from roboflow where it was annoted for to find Fire, smoke and default with boxes.</p>

<p>Link to Fire and Smoke detection <strong><em><a href="https://github.com/Abonia1/YOLOv8-Fire-and-Smoke-Detection/tree/main/datasets/fire-8">dataset</a>.</em></strong></p>

<h2 id="model-training-and-evaluation">Model Training and Evaluation</h2>

<p>The process of training YOLOv8 on a custom dataset involves several steps. First, a dataset of images containing fire and smoke should be collected and annotated. This involves marking the regions of interest in each image that correspond to fire and smoke. The annotated images can then be split into a training set and a validation set for use in the training process.I have useed <strong><em><a href="https://roboflow.com/">roboflow</a></em></strong> for the annotated dataset.</p>

<p><strong>Complete code in Colab Notebook:</strong></p>

<p><strong><a href="https://gist.github.com/Abonia1/eac0e5db887855bbab7875b914ebf429#file-train-yolov8-early-fire-smoke-detection-on-custom-dataset-ipynb"> Notebook Link </a></strong></p>

<script src="https://gist.github.com/Abonia1/eac0e5db887855bbab7875b914ebf429.js"></script>

<p>Once the model is trained, it can be evaluated on the validation set to measure its performance. This can be done by running the evaluation script, which outputs the mean Average Precision (mAP) score for the validation set. The mAP score is a measure of how well the model is able to detect the objects of interest in the validation set.</p>

<p><img src="https://cdn-images-1.medium.com/max/6000/1*8bM1hbqo6yO3-431pTaTAg.png" alt="Source-Image by Author: Confusion Matrix" /></p>

<p><img src="https://cdn-images-1.medium.com/max/4800/1*3-wM1-LMHxSb8PexVczhFg.png" alt="Source-Image by Author: Train,test,validation loss and mAP" /></p>

<p><img src="https://cdn-images-1.medium.com/max/4500/1*xGI8pDGGspE-stuyTfNLvg.png" alt="Source-Image by Author: PR curve" /></p>

<p>Finally, the trained model can be used for inference on new images/videos. This can be done by running the inference script and providing it with the path to an image. The script will display the detected fire and smoke regions on the input image.</p>

<h2 id="inference">Inference</h2>

<p>Sample Results :</p>

<p><img src="https://cdn-images-1.medium.com/max/3840/1*MlsDxcNfuYKBN88T-Y7TzA.jpeg" alt="" /></p>

<p><img src="https://cdn-images-1.medium.com/max/3840/1*0cI6BGRgIkBa7cMOHsn3CA.jpeg" alt="" /></p>

<h2 id="application-area"><strong>Application Area</strong></h2>

<p>Fire and smoke detection have a wide range of applications, including:</p>

<p>1.<strong>Residential and commercial buildings</strong>: Fire and smoke detectors are mandatory in all buildings to ensure the safety of residents and workers. They can alert people in case of a fire and give them enough time to evacuate the building.</p>

<ol>
  <li><strong>Industrial facilities</strong>: Fire and smoke detectors are crucial in industrial facilities where dangerous chemicals or flammable materials are stored. They can prevent fire outbreaks and minimize damage to property and the environment.</li>
</ol>

<p>3.<strong>Public transportation</strong>: Fire and smoke detectors are essential in public transportation such as buses, trains, and airplanes. They can ensure the safety of passengers and prevent disasters.</p>

<ol>
  <li>
    <p><strong>Power plants</strong>: Fire and smoke detectors are used in power plants to detect and prevent fires that can cause damage to critical equipment and disrupt power supply.</p>
  </li>
  <li>
    <p><strong>Military and defense</strong>: Fire and smoke detectors are used in military and defense installations to detect and prevent fires that can damage sensitive equipment and compromise national security.</p>
  </li>
  <li>
    <p><strong>Mining and oil and gas industries:</strong> Fire and smoke detectors are used in mines and oil and gas facilities to detect and prevent fires that can pose a risk to workers and the environment.</p>
  </li>
  <li>
    <p><strong>Forest Fire and Wildfire</strong>: Detect and alert in-case of forest fires.It helps to take the necessary steps to prevent forest fire and its negative effects.</p>
  </li>
</ol>

<p>In addition to these applications, fire and smoke detection systems are also used in museums, archives, and data centers to protect valuable and irreplaceable items. Overall, fire and smoke detection is a crucial component of public safety and disaster preparedness.</p>

<h2 id="conclusion">Conclusion</h2>

<p>YOLOv8 is a powerful tool for building fire and smoke detection systems using custom datasets. With its speed and accuracy, it is a promising solution for real-world applications that require fast and efficient fire and smoke detection.</p>

<p>Connect with me on <strong><a href="https://www.linkedin.com/in/aboniasojasingarayar/">Linkedin</a></strong></p>

<p>Find me on <strong><a href="https://github.com/Abonia1">Github</a></strong></p>

<p>Visit my technical channel on <strong><a href="https://www.youtube.com/@AboniaSojasingarayar">Youtube</a></strong></p>

<p>Support: <strong><a href="https://www.buymeacoffee.com/abonia">Buy me a Cofee/Chai</a></strong></p>]]></content><author><name>Abonia Sojasingarayar</name><email>aboniaa@gmail.com</email></author><category term="blogs" /><summary type="html"><![CDATA[Fine-Tune YOLOv8 Fire-and-Smoke-Detection on custom data]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cdn-images-1.medium.com/max/2000/1*HVm63Me_kPRNy0vxekCabw.gif" /><media:content medium="image" url="https://cdn-images-1.medium.com/max/2000/1*HVm63Me_kPRNy0vxekCabw.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>